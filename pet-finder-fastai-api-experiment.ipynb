{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.tabular import *\n",
    "from utils import *\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score,confusion_matrix,mean_squared_error,accuracy_score,make_scorer\n",
    "\n",
    "from fastai.callbacks.tracker import *\n",
    "\n",
    "path = Path.cwd()/'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.text.data import _join_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (14990, 151)\n",
      "X_test shape: (3948, 149)\n"
     ]
    }
   ],
   "source": [
    "train,test = preprocess_data_tabular(for_nn=True,drop_description=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentiment_document_magnitude',\n",
       " 'sentiment_magnitude',\n",
       " 'sentiment_document_score',\n",
       " 'sentiment_score',\n",
       " 'metadata_metadata_annots_score_MEAN',\n",
       " 'metadata_metadata_annots_score_SUM',\n",
       " 'metadata_metadata_color_score_MEAN',\n",
       " 'metadata_metadata_color_score_SUM',\n",
       " 'metadata_metadata_color_pixelfrac_MEAN',\n",
       " 'metadata_metadata_color_pixelfrac_SUM',\n",
       " 'metadata_metadata_crop_conf_MEAN',\n",
       " 'metadata_metadata_crop_conf_SUM',\n",
       " 'metadata_metadata_crop_importance_MEAN',\n",
       " 'metadata_metadata_crop_importance_SUM',\n",
       " 'image_size_sum',\n",
       " 'image_size_mean',\n",
       " 'image_size_std',\n",
       " 'width_sum',\n",
       " 'width_mean',\n",
       " 'width_std',\n",
       " 'height_sum',\n",
       " 'height_mean',\n",
       " 'height_std']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns[train.isna().any()].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_val_idxs():\n",
    "    seed=42\n",
    "    np.random.seed(seed)\n",
    "    rIDs = np.random.permutation(train.RescuerID.unique()).tolist()\n",
    "    \n",
    "    val_len = int(.2 * len(train.RescuerID.unique()) + 1)\n",
    "    train_idxs=[]\n",
    "    val_idxs=[]\n",
    "    for i in range(5):\n",
    "        val_rIDs = rIDs[val_len*i:val_len*(i+1)]\n",
    "        train_rIDs=rIDs[0:val_len*i] + rIDs[val_len*(i+1):]\n",
    "        train_idxs.append(train[train.RescuerID.isin(train_rIDs)].index.values)\n",
    "        val_idxs.append(train[train.RescuerID.isin(val_rIDs)].index.values)\n",
    "    return train_idxs,val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([    8,    13,    16,    19, ..., 14973, 14978, 14985, 14989]),\n",
       " array([   15,    21,    26,    27, ..., 14963, 14971, 14983, 14984]),\n",
       " array([    0,     6,     7,    24, ..., 14967, 14975, 14982, 14986]),\n",
       " array([    1,     2,    11,    12, ..., 14976, 14980, 14981, 14988]),\n",
       " array([    3,     4,     5,     9, ..., 14968, 14977, 14979, 14987])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs,val_idxs = generate_val_idxs()\n",
    "val_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14990, 167)\n",
      "(3948, 165)\n"
     ]
    }
   ],
   "source": [
    "train,test=mean_encoding(train,test,train_idxs,val_idxs,for_nn=True)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14990, 166)\n",
      "(3948, 165)\n"
     ]
    }
   ],
   "source": [
    "train=train.drop(['RescuerID'],axis=1)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of continuous feas: 145\n",
      "# of categorical feas: 19\n"
     ]
    }
   ],
   "source": [
    "cat_names=['Type','Breed1', 'Breed2','Gender','Color1', 'Color2', 'Color3',\n",
    "          'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed','Sterilized', 'Health','State','m1_label_description',\n",
    "           'Is_Sterilized','Is_Free','IsMandarin','IsMalay']\n",
    "# cont_names= list(set(train.columns) - set(cat_names) - {'AdoptionSpeed'})\n",
    "cont_names= list(set(train.columns) - set(cat_names) - {'AdoptionSpeed','Description'})\n",
    "print(f'# of continuous feas: {len(cont_names)}')\n",
    "print(f'# of categorical feas: {len(cat_names)}')\n",
    "dep_var = 'AdoptionSpeed'\n",
    "procs = [FillMissing,Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_cols=['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_names) + len(cont_names) + 2 == train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learner(data,layers,save_name='best_nn',emb_drop=0.0,ps=None):\n",
    "    return tabular_learner(data, layers=layers, metrics=[root_mean_squared_error],emb_drop=emb_drop,ps=ps,\n",
    "#                         callback_fns=[partial(EarlyStoppingCallback, monitor='quadratic_kappa_score',mode='max', min_delta=0.01, patience=3)]\n",
    "                           callback_fns=[partial(SaveModelCallback, monitor='root_mean_squared_error',mode='min',every='improvement',name=save_name)],\n",
    "                           \n",
    "                          )\n",
    "def get_learner_no_cb(data,layers,emb_drop=0.0,ps=None):\n",
    "    return tabular_learner(data, layers=layers, emb_drop=emb_drop,ps=ps,metrics=[root_mean_squared_error])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tabular databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_databunch(k=0,bs=64,val_idxs=val_idxs):\n",
    "    reset_seed()\n",
    "    data = (TabularList.from_df(train, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                           .split_by_idx(val_idxs[k])\n",
    "                           .label_from_df(cols=dep_var)\n",
    "                           .add_test(TabularList.from_df(test, path=path, cat_names=cat_names, cont_names=cont_names))\n",
    "                           .databunch(bs=bs))\n",
    "    return data\n",
    "\n",
    "l2str= lambda layers: '_'.join([str(l) for l in layers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get text databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_databunch(bs=64):\n",
    "    X_train = train[['Description','AdoptionSpeed']].copy()\n",
    "    X_test = test['Description'].copy()\n",
    "    \n",
    "    data_lm = load_data(path, 'tmp_lm.pkl', bs=bs)\n",
    "    \n",
    "\n",
    "    X_train.AdoptionSpeed = X_train.AdoptionSpeed.astype('int32')\n",
    "    \n",
    "    reset_seed()\n",
    "    data_clas = (TextList.from_df(X_train,path, vocab=data_lm.vocab)\n",
    "                 #grab all the text files in path\n",
    "                 .split_by_idx(val_idxs[0])\n",
    "                 #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "                 .label_from_df(cols='AdoptionSpeed',label_cls=FloatList)\n",
    "                 #label them all with their folders\n",
    "                 .add_test(TabularList.from_df(X_test, path=path))\n",
    "                 .databunch(bs=bs))\n",
    "\n",
    "    # data_clas.save('data_clas.pkl')\n",
    "    return data_clas\n",
    "#     return load_data(path, 'data_clas.pkl', bs=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ohmeow/medium-finding-data-block-nirvana/blob/master/yelp-00-custom-itemlist.ipynb\n",
    "\n",
    "Medium:\n",
    "\n",
    "https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete code block here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularText(TabularLine):\n",
    "    \"Item's that include both tabular data(`conts` and `cats`) and textual data (numericalized `ids`)\"\n",
    "    \n",
    "    def __init__(self, cats, conts, cat_classes, col_names, txt_ids, txt_cols, txt_string):\n",
    "        # tabular\n",
    "        super().__init__(cats, conts, cat_classes, col_names)\n",
    "\n",
    "        # add the text bits\n",
    "        self.text_ids = txt_ids\n",
    "        self.text_cols = txt_cols\n",
    "        self.text = txt_string\n",
    "        \n",
    "        # append numericalted text data to your input (represents your X values that are fed into your model)\n",
    "        # self.data = [tensor(cats), tensor(conts), tensor(txt_ids)]\n",
    "        self.data += [ np.array(txt_ids, dtype=np.int64) ]\n",
    "        self.obj = self.data\n",
    "        \n",
    "    def __str__(self):\n",
    "        res = super().__str__() + f'Text: {self.text}'\n",
    "        return res\n",
    "\n",
    "class TabularTextProcessor(TabularProcessor):\n",
    "    # The processors are called at the end of the labelling to apply some kind of function on your items. \n",
    "    # The default processor of the inputs can be overriden by passing a processor in the kwargs when creating the ItemList, \n",
    "    # the default processor of the targets can be overriden by passing a processor in the kwargs of the labelling function.\n",
    "    def __init__(self, ds:ItemList=None, procs=None, \n",
    "                 #tokenize processor args\n",
    "                 tokenizer:Tokenizer=None, chunksize:int=10000, # no mark_fields\n",
    "                 include_bos:bool=True, include_eos:bool=False, #include_bos, include_eos for def proces\n",
    "                 \n",
    "                 # numericalize processor args\n",
    "                 vocab:Vocab=None, max_vocab:int=60000, min_freq:int=2):\n",
    "        super().__init__(ds, procs)\n",
    "        \n",
    "        #tokenize\n",
    "        self.tokenizer, self.chunksize = ifnone(tokenizer, Tokenizer()), chunksize\n",
    "        self.include_bos, self.include_eos = include_bos, include_eos\n",
    "        \n",
    "        #numericalize\n",
    "        vocab = ifnone(vocab, ds.vocab if ds is not None else None)\n",
    "        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq\n",
    "        \n",
    "    # process a single item in a dataset\n",
    "    # NOTE: THIS IS METHOD HAS NOT BEEN TESTED AT THIS POINT (WILL COVER IN A FUTURE ARTICLE)\n",
    "    def process_one(self, item):\n",
    "#         # process tabular data (copied form tabular.data)\n",
    "#         df = pd.DataFrame([item, item])\n",
    "#         for proc in self.procs: proc(df, test=True)\n",
    "            \n",
    "#         if len(self.cat_names) != 0:\n",
    "#             codes = np.stack([c.cat.codes.values for n,c in df[self.cat_names].items()], 1).astype(np.int64) + 1\n",
    "#         else: \n",
    "#             codes = [[]]\n",
    "            \n",
    "#         if len(self.cont_names) != 0:\n",
    "#             conts = np.stack([c.astype('float32').values for n,c in df[self.cont_names].items()], 1)\n",
    "#         else: \n",
    "#             conts = [[]]\n",
    "            \n",
    "#         classes = None\n",
    "#         col_names = list(df[self.cat_names].columns.values) + list(df[self.cont_names].columns.values)\n",
    "        \n",
    "#         # process textual data\n",
    "#         if len(self.text_cols) != 0:\n",
    "#             txt = _join_texts(df[self.text_cols].values, (len(self.text_cols) > 1))\n",
    "#             txt_toks = self.tokenizer._process_all_1(txt)[0]\n",
    "#             text_ids = np.array(self.vocab.numericalize(txt_toks), dtype=np.int64)\n",
    "#         else:\n",
    "#             txt_toks, text_ids = None, [[]]\n",
    "            \n",
    "#         # return ItemBase\n",
    "#         return MixedTabularLine(codes[0], conts[0], classes, col_names, text_ids, self.txt_cols, txt_toks)\n",
    "        pass\n",
    "    # processes the entire dataset\n",
    "    def process(self, ds):\n",
    "        '''\n",
    "        ds is itembase\n",
    "        '''\n",
    "        # process tabular data and then set \"preprocessed=False\" since we still have text data possibly\n",
    "        super().process(ds)\n",
    "        ds.preprocessed = False\n",
    "        \n",
    "        # process text data from column(s) containing text\n",
    "        if len(ds.text_cols) != 0:\n",
    "            texts = _join_texts(ds.inner_df[ds.text_cols].values, (len(ds.text_cols) > 1), self.include_bos, self.include_eos)\n",
    "\n",
    "            # tokenize (set = .text)\n",
    "            tokens = []\n",
    "            for i in progress_bar(range(0, len(ds), self.chunksize), leave=False):\n",
    "                tokens += self.tokenizer.process_all(texts[i:i+self.chunksize])\n",
    "            ds.text = tokens\n",
    "            \n",
    "            # numericalize \n",
    "            # set/build vocab\n",
    "            if self.vocab is None: self.vocab = Vocab.create(ds.text, self.max_vocab, self.min_freq)\n",
    "            ds.vocab = self.vocab\n",
    "            ds.text_ids = [ np.array(self.vocab.numericalize(toks), dtype=np.int64) for toks in ds.text ]\n",
    "        else:\n",
    "            ds.text, ds.vocab, ds.text_ids = None, None, []\n",
    "            \n",
    "        ds.preprocessed = True\n",
    "        \n",
    "        \n",
    "def mixed_tabular_pad_collate(samples:BatchSamples, \n",
    "                              pad_idx:int=1, pad_first:bool=True) -> Tuple[LongTensor, LongTensor]:\n",
    "    \"Function that collect samples and adds padding.\"\n",
    "    # we need to add padding to the column with the text ids in order to ensure \n",
    "    # a square matrix per batch before integrating the text bits with the tabular.\n",
    "    \n",
    "\n",
    "    samples = to_data(samples)\n",
    "    max_len = max([len(s[0][-1]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "   \n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: \n",
    "            res[i,-len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "        else:         \n",
    "            res[i,:len(s[0][-1]):] = LongTensor(s[0][-1])\n",
    "            \n",
    "        # replace the text_ids array (the last thing in the inputs) with the padded tensor matrix\n",
    "        s[0][-1] = res[i]\n",
    "    \n",
    "\n",
    "    # for the inputs, return a list containing 3 elements: a list of cats, a list of conts, and a list of text_ids\n",
    "    # also include tensor list of classes\n",
    "    return [torch.stack(x) for x in zip(*[s[0] for s in samples])],tensor(np.array([s[1] for s in samples]))\n",
    "\n",
    "\n",
    "# each \"ds\" is of type LabelList(Dataset)\n",
    "class TabularTextDataBunch(DataBunch):\n",
    "    @classmethod\n",
    "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs=64, \n",
    "               pad_idx=1, pad_first=True, no_check:bool=False, **kwargs) -> DataBunch:\n",
    "        \n",
    "        # only thing we're doing here is setting the collate_fn = to our new \"pad_collate\" method above\n",
    "        collate_fn = partial(mixed_tabular_pad_collate, pad_idx=pad_idx, pad_first=pad_first)\n",
    "        \n",
    "        return super().create(train_ds, valid_ds, test_ds, path=path, bs=bs,**kwargs)\n",
    "\n",
    "    \n",
    "    \n",
    "class TabularTextList(TabularList):\n",
    "    \"A custom `ItemList` that merges tabular data along with textual data\"\n",
    "    \n",
    "    _item_cls = TabularText\n",
    "    _processor = TabularTextProcessor\n",
    "    _bunch = TabularTextDataBunch\n",
    "    \n",
    "    def __init__(self, items:Iterator, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                 text_cols=None, vocab:Vocab=None, pad_idx:int=1, \n",
    "                 procs=None, **kwargs) -> 'MixedTabularList':\n",
    "        super().__init__(items, cat_names, cont_names, procs, **kwargs)\n",
    "        \n",
    "        self.cols = [] if cat_names == None else cat_names.copy()\n",
    "        if cont_names: self.cols += cont_names.copy()\n",
    "        if txt_cols: self.cols += text_cols.copy()\n",
    "        \n",
    "        # from TextList\n",
    "        self.text_cols, self.vocab, self.pad_idx = text_cols, vocab, pad_idx\n",
    "        \n",
    "        # add any ItemList state into \"copy_new\" that needs to be copied each time \"new()\" is called; \n",
    "        # your ItemList acts as a prototype for training, validation, and/or test ItemList instances that\n",
    "        # are created via ItemList.new()\n",
    "        self.copy_new += ['text_cols', 'vocab', 'pad_idx']\n",
    "        \n",
    "        self.preprocessed = False\n",
    "        \n",
    "    # defines how to construct an ItemBase from the data in the ItemList.items array\n",
    "    def get(self, i):\n",
    "        if not self.preprocessed: \n",
    "            return self.inner_df.iloc[i][self.cols] if hasattr(self, 'inner_df') else self.items[i]\n",
    "        \n",
    "        codes = [] if self.codes is None else self.codes[i]\n",
    "        conts = [] if self.conts is None else self.conts[i]\n",
    "        \n",
    "        #from TextList\n",
    "        text_ids = [] if self.text_ids is None else self.text_ids[i]\n",
    "        text_string = None if self.text_ids is None else self.vocab.textify(self.text_ids[i])\n",
    "        \n",
    "        return self._item_cls(codes, conts, self.classes, self.col_names, text_ids, self.text_cols, text_string)\n",
    "    \n",
    "    # this is the method that is called in data.show_batch(), learn.predict() or learn.show_results() \n",
    "    # to transform a pytorch tensor back in an ItemBase. \n",
    "    # in a way, it does the opposite of calling ItemBase.data. It should take a tensor t and return \n",
    "    # the same kind of thing as the get method.\n",
    "    def reconstruct(self, t:Tensor):\n",
    "        # TODO?\n",
    "        idx_min = (t[2] != self.pad_idx).nonzero().min()\n",
    "        idx_max = (t[2] != self.pad_idx).nonzero().max()\n",
    "        return self._item_cls(t[0], t[1], self.classes, self.col_names, \n",
    "                              t[2][idx_min:idx_max], self.text_cols, self.vocab.textify(t[2][idx_min:idx_max]))\n",
    "        \n",
    "#         return self._item_cls(t[0], t[1], self.classes, self.col_names, \n",
    "#                               t[2], self.text_cols, self.vocab.textify(t[2])) \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    # tells fastai how to display a custom ItemBase when data.show_batch() is called\n",
    "    def show_xys(self, xs, ys) -> None:\n",
    "        \"Show the `xs` (inputs) and `ys` (targets).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        display(HTML('TABULAR:<br>'))\n",
    "        super().show_xys(xs, ys)\n",
    "        \n",
    "        # show text        \n",
    "        display(HTML('TEXT:<br>'))        \n",
    "        names = ['text', 'target']\n",
    "        items = []\n",
    "        for i, (x,y) in enumerate(zip(xs,ys)):\n",
    "            res = []\n",
    "            res.append(' '.join([ f'{tok}({self.vocab.stoi[tok]})' \n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ]))\n",
    "                \n",
    "            res.append(str(y))\n",
    "            items.append(res)\n",
    "\n",
    "        items = np.array(items)\n",
    "        df = pd.DataFrame({n:items[:,i] for i,n in enumerate(names)})\n",
    "        with pd.option_context('display.max_colwidth', -1):\n",
    "            display(HTML(df.to_html(index=False)))\n",
    "        \n",
    "    # tells fastai how to display a custom ItemBase when learn.show_results() is called\n",
    "    def show_xyzs(self, xs, ys, zs):\n",
    "        \"Show `xs` (inputs), `ys` (targets) and `zs` (predictions).\"\n",
    "        from IPython.display import display, HTML\n",
    "        \n",
    "        # show tabular\n",
    "        display(HTML('TABULAR:<br>'))\n",
    "        super().show_xyzs(xs, ys, zs)\n",
    "        \n",
    "        # show text        \n",
    "        display(HTML('TEXT:<br>'))        \n",
    "        names = ['text', 'target','pred']\n",
    "        items = []\n",
    "        for i, (x,y,z) in enumerate(zip(xs,ys,zs)):\n",
    "            res = []\n",
    "            res.append(' '.join([ f'{tok}({self.vocab.stoi[tok]})' \n",
    "                              for tok in x.text.split() if (not self.vocab.stoi[tok] == self.pad_idx) ]))\n",
    "                \n",
    "            res += [str(y),str(z)]\n",
    "            items.append(res)\n",
    "            \n",
    "        items = np.array(items)\n",
    "        df = pd.DataFrame({n:items[:,i] for i,n in enumerate(names)})\n",
    "        with pd.option_context('display.max_colwidth', -1):\n",
    "            display(HTML(df.to_html(index=False)))\n",
    "    \n",
    "        \n",
    "    @classmethod\n",
    "    def from_df(cls, df:DataFrame, cat_names:OptStrList=None, cont_names:OptStrList=None, \n",
    "                text_cols=None, vocab=None, procs=None, **kwargs) -> 'ItemList':\n",
    "        \n",
    "        return cls(items=range(len(df)), cat_names=cat_names, cont_names=cont_names, \n",
    "                   text_cols=text_cols, vocab=vocab, procs=procs, inner_df=df.copy(), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tabulartext_databunch(k=0,bs=64,val_idxs=val_idxs):\n",
    "    data_lm = load_data(path, 'tmp_lm.pkl', bs=bs)\n",
    "    collate_fn = partial(mixed_tabular_pad_collate, pad_idx=1, pad_first=True)\n",
    "    reset_seed()\n",
    "    return (TabularTextList.from_df(train, cat_names, cont_names, txt_cols, vocab=data_lm.vocab, procs=procs, path=path)\n",
    "                            .split_by_idx(val_idxs[k])\n",
    "                            .label_from_df(cols=dep_var)\n",
    "                            .add_test(TabularTextList.from_df(test, cat_names, cont_names, txt_cols,path=path))\n",
    "                            .databunch(bs=bs,collate_fn=collate_fn, no_check=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tab and text learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tabular_learner(data:DataBunch, layers:Collection[int], emb_szs:Dict[str,int]=None, metrics=None,\n",
    "#         ps:Collection[float]=None, emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, **learn_kwargs):\n",
    "#     \"Get a `Learner` using `data`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n",
    "#     emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
    "#     model = TabularModel(emb_szs, len(data.cont_names), out_sz=data.c, layers=layers, ps=ps, emb_drop=emb_drop,\n",
    "#                          y_range=y_range, use_bn=use_bn)\n",
    "#     return Learner(data, model, metrics=metrics, **learn_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearTabularTextClassifier(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, rnn_lin_layers:Collection[int], ps_lin_ftrs:Collection[float],\n",
    "                 # tabular params inputs\n",
    "                 emb_szs,n_cont,n_class,layers,ps,emb_drop,y_range,use_bn,bn_final):\n",
    "        # rnn_lin_layers: [1200, 50, 1]\n",
    "        # ps_lin_ftrs: [0.4 (from output_p, layer1200), 0.1 for layer50]\n",
    "        \n",
    "        super().__init__()\n",
    "        # text layers\n",
    "        mod_layers = []\n",
    "        activs = [nn.ReLU(inplace=True)] * (len(rnn_lin_layers) - 2) + [None]\n",
    "        for n_in,n_out,p,actn in zip(rnn_lin_layers[:-1],rnn_lin_layers[1:], ps_lin_ftrs, activs):\n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "        mod_layers = mod_layers[:-1] # exclude the last linear output\n",
    "#         self.layers = nn.Sequential(*mod_layers[:-1]) \n",
    "    \n",
    "    \n",
    "        #tabular layers\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "                \n",
    "        # embedding stuff\n",
    "        self.embeds = nn.ModuleList([embedding(ni, nf) for ni,nf in emb_szs])\n",
    "        self.emb_drop = nn.Dropout(emb_drop) # drop for embedding\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont) # bn for continuous features\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds) # total length of cat embeddings\n",
    "        \n",
    "               \n",
    "        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n",
    "        sizes = self.get_sizes(layers, rnn_lin_layers[-2],n_class) # [343, 222, 111, 1] ->convert to [343+ 50, 222, 111, 1]        \n",
    "        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None] # [ReLU(inplace), ReLU(inplace), None]\n",
    "        \n",
    "        # self.layers can stay the same\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        \n",
    "#         print(mod_layers + layers)\n",
    "        self.layers = nn.Sequential(*(mod_layers + layers))\n",
    "\n",
    "        \n",
    "        \n",
    "    def get_sizes(self, layers, rnn_lin_layer, out_sz):\n",
    "        # concatenate cat,conts of tabular and rnn lin layer \n",
    "        return [self.n_emb + self.n_cont + rnn_lin_layer] + layers + [out_sz]\n",
    "    \n",
    "#     def forward(self, input:Tuple[Tensor,Tensor, Tensor])->Tuple[Tensor,Tensor,Tensor]:\n",
    "#         pdb.set_trace()\n",
    "#         raw_outputs,outputs,mask = input\n",
    "#         output = outputs[-1]\n",
    "#         avg_pool = output.masked_fill(mask[:,:,None], 0).mean(dim=1)\n",
    "#         avg_pool *= output.size(1) / (output.size(1)-mask.float().sum(dim=1))[:,None]\n",
    "#         max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "#         x = torch.cat([output[:,-1], max_pool, avg_pool], 1)\n",
    "#         x = self.layers(x)\n",
    "#         return x, raw_outputs, outputs # TODO: why do we need raw_outputs and outputs?\n",
    "#         return None\n",
    "    \n",
    "    def forward(self, input:Tuple[Tensor,Tensor,Tensor,Tensor,Tensor]):\n",
    "        x_cat,x_cont,raw_outputs,outputs,mask = input\n",
    "        # text\n",
    "        output = outputs[-1]\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).mean(dim=1)\n",
    "        avg_pool *= output.size(1) / (output.size(1)-mask.float().sum(dim=1))[:,None]\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x_text = torch.cat([output[:,-1], max_pool, avg_pool], 1) #(bs,1200) for AWD LSTM\n",
    "        \n",
    "        # tabular\n",
    "        if self.n_emb != 0:\n",
    "            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_drop(x)\n",
    "        if self.n_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n",
    "            \n",
    "        x = torch.cat([x_text,x],1)\n",
    "        x = self.layers(x)\n",
    "        if self.y_range is not None:\n",
    "            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n",
    "        return x,raw_outputs,outputs # TODO: why do we need raw_outputs and outputs?\n",
    "\n",
    "class MultiBatchMixEncoder(MultiBatchEncoder):\n",
    "    \"Create an encoder over `module` that can process a full sentence.\"\n",
    "    def __init__(self, bptt:int, max_len:int, module:nn.Module, pad_idx:int=1):\n",
    "        super().__init__(bptt,max_len,module,pad_idx)\n",
    "\n",
    "    def forward(self, x_cat:Tensor,x_cont:Tensor,x_text:LongTensor):\n",
    "        bs,sl = x_text.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            r, o = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_len):\n",
    "                masks.append(input[:,i: min(i+self.bptt, sl)] == self.pad_idx)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "        return x_cat,x_cont,self.concat(raw_outputs),self.concat(outputs),torch.cat(masks,dim=1)\n",
    "\n",
    "                                \n",
    "def get_tabular_text_classifier(emb_szs:ListSizes, n_cont:int , n_class:int, layers:Collection[int], \n",
    "                                # text classifier params inputs\n",
    "                                arch:Callable, vocab_sz:int, bptt:int=70, max_len:int=20*70, config:dict=None, \n",
    "                                drop_mult:float=1., lin_ftrs:Collection[int]=None, ps_lin_ftrs:Collection[float]=None,pad_idx:int=1,\n",
    "                                # tabular params inputs\n",
    "                                ps:Collection[float]=None,emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False\n",
    "                               ) -> nn.Module:\n",
    "    \"Create a text classifier from `arch` and its `config`, maybe `pretrained`.\"\n",
    "    \n",
    "    # TODO: remove text.learner.*?\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    config = ifnone(config, meta['config_clas'].copy())\n",
    "    for k in config.keys(): \n",
    "        if k.endswith('_p'): config[k] *= drop_mult # drop_mult: multiply to different dropouts in AWD LSTM\n",
    "    if lin_ftrs is None: lin_ftrs = [50]\n",
    "    if ps_lin_ftrs is None:  ps_lin_ftrs = [0.1]\n",
    "    \n",
    "    rnn_lin_layers = [config[meta['hid_name']] * 3] + lin_ftrs + [n_class] # [1200, 50, 1]\n",
    "    ps_lin_ftrs = [config.pop('output_p')] + ps_lin_ftrs #[0.4 (from output_p), 0.1 for layer50]\n",
    "    init = config.pop('init') if 'init' in config else None\n",
    "    encoder = MultiBatchMixEncoder(bptt, max_len, arch(vocab_sz, **config), pad_idx=pad_idx)\n",
    "    \n",
    "    tabtext_lin_model = PoolingLinearTabularTextClassifier(rnn_lin_layers, ps_lin_ftrs,\n",
    "                                                       # tabular params inputs\n",
    "                                                      emb_szs,n_cont,n_class,layers,ps,emb_drop,y_range,use_bn,bn_final)\n",
    "    final_model = SequentialRNN(encoder, tabtext_lin_model)\n",
    "    return final_model if init is None else final_model.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabtext_learner(data,layers:Collection[int], \n",
    "                    arch:Callable,\n",
    "                    # text classifier params inputs\n",
    "                    metrics=None,\n",
    "                    bptt:int=70, max_len:int=20*70, config:dict=None,                     \n",
    "                    drop_mult:float=1., lin_ftrs:Collection[int]=None, ps_lin_ftrs:Collection[float]=None,pad_idx:int=1,\n",
    "                    # tabular params inputs\n",
    "                    emb_szs:Dict[str,int]=None, ps:Collection[float]=None,emb_drop:float=0., \n",
    "                    y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False,pretrained:bool=True, **learn_kwargs):\n",
    "    emb_szs = data.get_emb_szs(ifnone(emb_szs, {}))\n",
    "    model = get_tabular_text_classifier(emb_szs,len(data.cont_names),data.c,layers,\n",
    "                                       arch,len(data.vocab.itos),bptt=bptt,max_len=max_len,config=config,\n",
    "                                        drop_mult=drop_mult, lin_ftrs=lin_ftrs,ps_lin_ftrs=ps_lin_ftrs,\n",
    "                                        pad_idx=pad_idx,ps=ps,emb_drop=emb_drop,y_range=y_range,\n",
    "                                       use_bn=use_bn,bn_final=bn_final)\n",
    "    #text\n",
    "    meta = text.learner._model_meta[arch]\n",
    "    learn = RNNLearner(data, model, metrics = metrics, split_func=meta['split_clas'], **learn_kwargs)\n",
    "    if pretrained:\n",
    "        if 'url' not in meta: \n",
    "            warn(\"There are no pretrained weights for that architecture yet!\")\n",
    "            return learn\n",
    "        model_path = untar_data(meta['url'], data=False)\n",
    "        fnames = [list(model_path.glob(f'*.{ext}'))[0] for ext in ['pth', 'pkl']]\n",
    "        learn.load_pretrained(*fnames, strict=False)\n",
    "        learn.freeze()\n",
    "    return learn\n",
    "\n",
    "#     learn = Learner(data,model,metrics=metrics,**learn_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabtext_db = get_tabulartext_databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabtext_learner(tabtext_db,[222,111],AWD_LSTM,metrics=[root_mean_squared_error],\n",
    "                       callback_fns=[partial(SaveModelCallback, monitor='root_mean_squared_error',mode='min',every='improvement',name='best_nn')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encoder('bs256-8e-awdlstm-enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True),\n",
       " functools.partial(<class 'fastai.callbacks.tracker.SaveModelCallback'>, monitor='root_mean_squared_error', mode='min', every='improvement', name='best_nn')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.callback_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_szs = tabtext_db.get_emb_szs({})\n",
    "# model = get_tabular_text_classifier(emb_szs,len(tabtext_db.cont_names),tabtext_db.c,[222,111],\n",
    "#                                    AWD_LSTM,len(tabtext_db.vocab.itos),lin_ftrs=[55,44],ps_lin_ftrs=[0.55,0.44],ps=[0.22,0.11],\n",
    "#                                    emb_drop=0.111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(10131, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(10131, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearTabTextClassifier(\n",
       "    (embeds): ModuleList(\n",
       "      (0): Embedding(3, 3)\n",
       "      (1): Embedding(165, 28)\n",
       "      (2): Embedding(123, 24)\n",
       "      (3): Embedding(4, 3)\n",
       "      (4): Embedding(8, 5)\n",
       "      (5): Embedding(8, 5)\n",
       "      (6): Embedding(7, 5)\n",
       "      (7): Embedding(5, 4)\n",
       "      (8): Embedding(4, 3)\n",
       "      (9): Embedding(4, 3)\n",
       "      (10): Embedding(4, 3)\n",
       "      (11): Embedding(4, 3)\n",
       "      (12): Embedding(4, 3)\n",
       "      (13): Embedding(14, 7)\n",
       "      (14): Embedding(76, 18)\n",
       "      (15): Embedding(3, 3)\n",
       "      (16): Embedding(3, 3)\n",
       "      (17): Embedding(3, 3)\n",
       "      (18): Embedding(3, 3)\n",
       "      (19): Embedding(3, 3)\n",
       "      (20): Embedding(3, 3)\n",
       "      (21): Embedding(3, 3)\n",
       "      (22): Embedding(3, 3)\n",
       "      (23): Embedding(3, 3)\n",
       "      (24): Embedding(3, 3)\n",
       "      (25): Embedding(3, 3)\n",
       "      (26): Embedding(3, 3)\n",
       "      (27): Embedding(3, 3)\n",
       "      (28): Embedding(3, 3)\n",
       "      (29): Embedding(3, 3)\n",
       "      (30): Embedding(3, 3)\n",
       "      (31): Embedding(3, 3)\n",
       "      (32): Embedding(3, 3)\n",
       "      (33): Embedding(3, 3)\n",
       "      (34): Embedding(3, 3)\n",
       "      (35): Embedding(3, 3)\n",
       "      (36): Embedding(3, 3)\n",
       "      (37): Embedding(3, 3)\n",
       "      (38): Embedding(3, 3)\n",
       "      (39): Embedding(3, 3)\n",
       "      (40): Embedding(3, 3)\n",
       "      (41): Embedding(3, 3)\n",
       "    )\n",
       "    (emb_drop): Dropout(p=0.111)\n",
       "    (bn_cont): BatchNorm1d(145, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.4)\n",
       "      (2): Linear(in_features=1200, out_features=55, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.55)\n",
       "      (6): Linear(in_features=55, out_features=44, bias=True)\n",
       "      (7): ReLU(inplace)\n",
       "      (8): BatchNorm1d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): Dropout(p=0.44)\n",
       "      (10): Linear(in_features=387, out_features=222, bias=True)\n",
       "      (11): ReLU(inplace)\n",
       "      (12): BatchNorm1d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): Dropout(p=0.22)\n",
       "      (14): Linear(in_features=222, out_features=111, bias=True)\n",
       "      (15): ReLU(inplace)\n",
       "      (16): BatchNorm1d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (17): Dropout(p=0.11)\n",
       "      (18): Linear(in_features=111, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = get_text_classifier(AWD_LSTM, len(tabtext_db.vocab.itos), tabtext_db.c, bptt=70, max_len=70*20,\n",
    "#                                 drop_mult=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(10131, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(10131, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1150, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1150, 1150, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1150, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.4)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# emb_szs = tabtext_db.get_emb_szs({})\n",
    "# model = TabularModel(emba_szs, len(tabtext_db.cont_names), out_sz=tabtext_db.c, layers=[222,111],ps=[0.2,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(3, 3)\n",
       "    (1): Embedding(165, 28)\n",
       "    (2): Embedding(123, 24)\n",
       "    (3): Embedding(4, 3)\n",
       "    (4): Embedding(8, 5)\n",
       "    (5): Embedding(8, 5)\n",
       "    (6): Embedding(7, 5)\n",
       "    (7): Embedding(5, 4)\n",
       "    (8): Embedding(4, 3)\n",
       "    (9): Embedding(4, 3)\n",
       "    (10): Embedding(4, 3)\n",
       "    (11): Embedding(4, 3)\n",
       "    (12): Embedding(4, 3)\n",
       "    (13): Embedding(14, 7)\n",
       "    (14): Embedding(76, 18)\n",
       "    (15): Embedding(3, 3)\n",
       "    (16): Embedding(3, 3)\n",
       "    (17): Embedding(3, 3)\n",
       "    (18): Embedding(3, 3)\n",
       "    (19): Embedding(3, 3)\n",
       "    (20): Embedding(3, 3)\n",
       "    (21): Embedding(3, 3)\n",
       "    (22): Embedding(3, 3)\n",
       "    (23): Embedding(3, 3)\n",
       "    (24): Embedding(3, 3)\n",
       "    (25): Embedding(3, 3)\n",
       "    (26): Embedding(3, 3)\n",
       "    (27): Embedding(3, 3)\n",
       "    (28): Embedding(3, 3)\n",
       "    (29): Embedding(3, 3)\n",
       "    (30): Embedding(3, 3)\n",
       "    (31): Embedding(3, 3)\n",
       "    (32): Embedding(3, 3)\n",
       "    (33): Embedding(3, 3)\n",
       "    (34): Embedding(3, 3)\n",
       "    (35): Embedding(3, 3)\n",
       "    (36): Embedding(3, 3)\n",
       "    (37): Embedding(3, 3)\n",
       "    (38): Embedding(3, 3)\n",
       "    (39): Embedding(3, 3)\n",
       "    (40): Embedding(3, 3)\n",
       "    (41): Embedding(3, 3)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.0)\n",
       "  (bn_cont): BatchNorm1d(145, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=343, out_features=222, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): BatchNorm1d(222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.2)\n",
       "    (4): Linear(in_features=222, out_features=111, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.1)\n",
       "    (8): Linear(in_features=111, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete tabular text databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabtext_db = get_tabulartext_databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TABULAR:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>State</th>\n",
       "      <th>m1_label_description</th>\n",
       "      <th>Is_Sterilized</th>\n",
       "      <th>Is_Free</th>\n",
       "      <th>IsMandarin</th>\n",
       "      <th>IsMalay</th>\n",
       "      <th>width_mean_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN_na</th>\n",
       "      <th>image_size_std_na</th>\n",
       "      <th>width_std_na</th>\n",
       "      <th>sentiment_magnitude_na</th>\n",
       "      <th>height_mean_na</th>\n",
       "      <th>metadata_metadata_annots_score_SUM_na</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN_na</th>\n",
       "      <th>image_size_mean_na</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN_na</th>\n",
       "      <th>sentiment_document_magnitude_na</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM_na</th>\n",
       "      <th>image_size_sum_na</th>\n",
       "      <th>height_sum_na</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM_na</th>\n",
       "      <th>sentiment_score_na</th>\n",
       "      <th>metadata_metadata_color_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM_na</th>\n",
       "      <th>height_std_na</th>\n",
       "      <th>metadata_metadata_color_score_SUM_na</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN_na</th>\n",
       "      <th>width_sum_na</th>\n",
       "      <th>sentiment_document_score_na</th>\n",
       "      <th>TFIDF_Description_5</th>\n",
       "      <th>RescuerID_age_mean</th>\n",
       "      <th>general health</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_15</th>\n",
       "      <th>potential for mouthiness</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Breed1_m1_vertex_y_mean</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_11</th>\n",
       "      <th>RescuerID_Fee_Mean</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>RescuerID_m1LabelScore_mean</th>\n",
       "      <th>TFIDF_Description_7</th>\n",
       "      <th>Breed1_Age_mean</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_14</th>\n",
       "      <th>TFIDF_sentiment_entities_10</th>\n",
       "      <th>tolerates hot weather</th>\n",
       "      <th>State_Pop</th>\n",
       "      <th>State_RescuerID_COUNT_sum</th>\n",
       "      <th>State_m1_label_score_mean</th>\n",
       "      <th>size</th>\n",
       "      <th>adapts well to apartment living</th>\n",
       "      <th>good for novice owners</th>\n",
       "      <th>drooling potential</th>\n",
       "      <th>TFIDF_sentiment_entities_14</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_2</th>\n",
       "      <th>Breed1_RescuerID_nunique</th>\n",
       "      <th>easy to train</th>\n",
       "      <th>tendency to bark or howl</th>\n",
       "      <th>TFIDF_sentiment_entities_1</th>\n",
       "      <th>image_size_std</th>\n",
       "      <th>State_Density</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_13</th>\n",
       "      <th>width_std</th>\n",
       "      <th>m1_dominant_red</th>\n",
       "      <th>TFIDF_Description_9</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>metadata_metadata_annots_score_SUM</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN</th>\n",
       "      <th>TFIDF_Description_2</th>\n",
       "      <th>TFIDF_Description_14</th>\n",
       "      <th>wanderlust potential</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_0</th>\n",
       "      <th>RescuerID_breed1_nunique</th>\n",
       "      <th>TFIDF_Description_3</th>\n",
       "      <th>image_size_mean</th>\n",
       "      <th>State_m1_vertex_y_mean</th>\n",
       "      <th>TFIDF_Description_6</th>\n",
       "      <th>State_Area</th>\n",
       "      <th>prey drive</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN</th>\n",
       "      <th>TFIDF_sentiment_entities_15</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>TFIDF_sentiment_entities_11</th>\n",
       "      <th>potential for playfulness</th>\n",
       "      <th>Age</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_10</th>\n",
       "      <th>Breed1_count</th>\n",
       "      <th>Breed1_m1_label_score_sum</th>\n",
       "      <th>m1_dominant_blue</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM</th>\n",
       "      <th>TFIDF_sentiment_entities_6</th>\n",
       "      <th>State_count</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>TFIDF_sentiment_entities_8</th>\n",
       "      <th>image_size_sum</th>\n",
       "      <th>affectionate with family</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_9</th>\n",
       "      <th>easy to groom</th>\n",
       "      <th>pet friendly</th>\n",
       "      <th>friendly toward strangers</th>\n",
       "      <th>TFIDF_sentiment_entities_9</th>\n",
       "      <th>tendency to vocalize</th>\n",
       "      <th>incredibly kid friendly dogs</th>\n",
       "      <th>height_sum</th>\n",
       "      <th>m1_dominant_pixel_frac</th>\n",
       "      <th>TFIDF_sentiment_entities_5</th>\n",
       "      <th>RescuerID_m1VertexY_mean</th>\n",
       "      <th>RescuerID_IsFree_Mean</th>\n",
       "      <th>intensity</th>\n",
       "      <th>State_Age_mean</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_12</th>\n",
       "      <th>State_GDP</th>\n",
       "      <th>m1_vertex_y</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM</th>\n",
       "      <th>TFIDF_Description_0</th>\n",
       "      <th>State_m1_label_score_sum</th>\n",
       "      <th>RescuerID_m1LabelScore_sum</th>\n",
       "      <th>RescuerID_Fee_Sum</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_5</th>\n",
       "      <th>TFIDF_sentiment_entities_4</th>\n",
       "      <th>TFIDF_sentiment_entities_12</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_7</th>\n",
       "      <th>potential for weight gain</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_3</th>\n",
       "      <th>metadata_metadata_color_score_MEAN</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM</th>\n",
       "      <th>TFIDF_Description_13</th>\n",
       "      <th>TFIDF_sentiment_entities_13</th>\n",
       "      <th>height_std</th>\n",
       "      <th>State_RescuerID_nunique</th>\n",
       "      <th>State_RescuerID_COUNT_mean</th>\n",
       "      <th>TFIDF_Description_8</th>\n",
       "      <th>sensitivity level</th>\n",
       "      <th>m1_vertex_x</th>\n",
       "      <th>name_length</th>\n",
       "      <th>Breed1_RescuerID_COUNT_sum</th>\n",
       "      <th>TFIDF_sentiment_entities_3</th>\n",
       "      <th>metadata_metadata_color_score_SUM</th>\n",
       "      <th>TFIDF_Description_10</th>\n",
       "      <th>TFIDF_Description_1</th>\n",
       "      <th>TFIDF_sentiment_entities_0</th>\n",
       "      <th>TFIDF_Description_12</th>\n",
       "      <th>energy level</th>\n",
       "      <th>m1_bounding_confidence</th>\n",
       "      <th>m1_dominant_score</th>\n",
       "      <th>exercise needs</th>\n",
       "      <th>dog friendly</th>\n",
       "      <th>RescuerID_COUNT</th>\n",
       "      <th>m1_bounding_importance</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_4</th>\n",
       "      <th>amount of shedding</th>\n",
       "      <th>TFIDF_Description_11</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN</th>\n",
       "      <th>tolerates being alone</th>\n",
       "      <th>TFIDF_sentiment_entities_7</th>\n",
       "      <th>Breed1_RescuerID_COUNT_mean</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_1</th>\n",
       "      <th>m1_dominant_green</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_8</th>\n",
       "      <th>width_sum</th>\n",
       "      <th>TFIDF_Description_4</th>\n",
       "      <th>kid friendly</th>\n",
       "      <th>tolerates cold weather</th>\n",
       "      <th>TFIDF_Description_15</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_6</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>m1_label_score</th>\n",
       "      <th>Breed1_m1_label_score_mean</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>TFIDF_sentiment_entities_2</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.2549</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>2.7438</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.3161</td>\n",
       "      <td>0.3228</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.0607</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.3582</td>\n",
       "      <td>-0.3003</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.5318</td>\n",
       "      <td>-0.8797</td>\n",
       "      <td>-1.0693</td>\n",
       "      <td>-0.4375</td>\n",
       "      <td>-0.8303</td>\n",
       "      <td>-1.0736</td>\n",
       "      <td>0.6044</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.2367</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.7828</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>-0.2550</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>-0.8706</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>0.7317</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>-0.7754</td>\n",
       "      <td>0.5891</td>\n",
       "      <td>3.1878</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.6022</td>\n",
       "      <td>1.0712</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-2.4276</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>0.2029</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>-0.6531</td>\n",
       "      <td>-0.6292</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>-0.0046</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.7940</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>-1.0823</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3692</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.6081</td>\n",
       "      <td>-0.4922</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.6921</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>-0.9616</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>1.3544</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-1.1636</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>-0.1920</td>\n",
       "      <td>-0.6448</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3429</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.3704</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.1087</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.6733</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-2.1347</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>1.4372</td>\n",
       "      <td>-0.8293</td>\n",
       "      <td>-1.1070</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>-0.4783</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.4454</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>-0.7934</td>\n",
       "      <td>-0.2633</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.3525</td>\n",
       "      <td>-0.5742</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.1767</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>2.2813</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>0.8607</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>3.6141</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>-1.0765</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>-1.5543</td>\n",
       "      <td>-1.4085</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-1.1138</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.0885</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>0.4751</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>-1.1127</td>\n",
       "      <td>-0.4062</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>-0.8588</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.3692</td>\n",
       "      <td>-1.3740</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-1.3210</td>\n",
       "      <td>-0.5044</td>\n",
       "      <td>-0.6810</td>\n",
       "      <td>0.4637</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-0.4818</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.6482</td>\n",
       "      <td>0.2639</td>\n",
       "      <td>0.4531</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-0.1980</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>-0.6990</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.0304</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-2.0590</td>\n",
       "      <td>-0.6528</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>0.6537</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>2.1340</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.7792</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3374</td>\n",
       "      <td>-0.4286</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>-0.1506</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>-0.7812</td>\n",
       "      <td>-0.9848</td>\n",
       "      <td>0.1229</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.7019</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>1.8634</td>\n",
       "      <td>1.8753</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>1.0284</td>\n",
       "      <td>-0.8360</td>\n",
       "      <td>0.5516</td>\n",
       "      <td>-0.4924</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>-0.3706</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.4188</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>2.4888</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>-0.8891</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>-0.6451</td>\n",
       "      <td>0.1877</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>-0.6008</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>0.8099</td>\n",
       "      <td>0.5191</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5670</td>\n",
       "      <td>-0.5076</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>0.2022</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.9212</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.2847</td>\n",
       "      <td>-0.1629</td>\n",
       "      <td>0.2856</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>-0.4237</td>\n",
       "      <td>1.8355</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.0280</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.6953</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-1.0581</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>-0.4636</td>\n",
       "      <td>-0.6268</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>-0.8748</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.1304</td>\n",
       "      <td>-0.6529</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-1.0142</td>\n",
       "      <td>-0.5044</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>-0.3995</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.0561</td>\n",
       "      <td>-0.5870</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.2530</td>\n",
       "      <td>-0.0717</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.4168</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.1872</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>-0.7621</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>2.2284</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.0133</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.8333</td>\n",
       "      <td>0.1629</td>\n",
       "      <td>-0.0948</td>\n",
       "      <td>-0.1453</td>\n",
       "      <td>-2.0504</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-1.3586</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3986</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>-0.1168</td>\n",
       "      <td>0.1268</td>\n",
       "      <td>-0.1511</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>2.2419</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>0.5448</td>\n",
       "      <td>-0.7978</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.6262</td>\n",
       "      <td>-0.2387</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>-0.1620</td>\n",
       "      <td>-0.8444</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>-0.1922</td>\n",
       "      <td>-0.3830</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.3994</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>1.8687</td>\n",
       "      <td>-0.4226</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.2103</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>1.1361</td>\n",
       "      <td>0.6666</td>\n",
       "      <td>-2.5701</td>\n",
       "      <td>-0.8747</td>\n",
       "      <td>-0.2455</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1314</td>\n",
       "      <td>1.5110</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>-0.2833</td>\n",
       "      <td>-0.9716</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.6903</td>\n",
       "      <td>-0.4410</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-1.8631</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.2847</td>\n",
       "      <td>-1.9524</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>2.2769</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>-0.4237</td>\n",
       "      <td>-1.0196</td>\n",
       "      <td>-0.1286</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>0.6969</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.6999</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.0049</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>-1.5035</td>\n",
       "      <td>-0.7988</td>\n",
       "      <td>-0.2199</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>-0.8707</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.3268</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.9989</td>\n",
       "      <td>-0.4059</td>\n",
       "      <td>1.2022</td>\n",
       "      <td>-0.7777</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.0561</td>\n",
       "      <td>-0.2466</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.5266</td>\n",
       "      <td>-4.0559</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>-1.2798</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.1880</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.8100</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.8333</td>\n",
       "      <td>0.7223</td>\n",
       "      <td>-0.0952</td>\n",
       "      <td>-0.4185</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.2182</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>1.1023</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>0.1284</td>\n",
       "      <td>-1.6291</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-1.0954</td>\n",
       "      <td>-0.7824</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-1.3585</td>\n",
       "      <td>-1.1636</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>-0.1650</td>\n",
       "      <td>-0.9066</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>-0.2912</td>\n",
       "      <td>5.2294</td>\n",
       "      <td>-0.8588</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.1210</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.4188</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.1544</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.9175</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.2123</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>-1.3678</td>\n",
       "      <td>3.5829</td>\n",
       "      <td>-0.9422</td>\n",
       "      <td>-0.5393</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>2.6744</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>-0.2104</td>\n",
       "      <td>-0.9773</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.5306</td>\n",
       "      <td>0.6874</td>\n",
       "      <td>1.2087</td>\n",
       "      <td>-0.2083</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.9212</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-2.1551</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>1.3044</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>-2.0212</td>\n",
       "      <td>-0.1631</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0.3234</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>-1.3688</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>-0.3672</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>1.1492</td>\n",
       "      <td>-0.8797</td>\n",
       "      <td>0.7545</td>\n",
       "      <td>1.4700</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>-0.2777</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-1.0039</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>-0.4528</td>\n",
       "      <td>-0.9143</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.2652</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>0.8179</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>1.3882</td>\n",
       "      <td>1.1315</td>\n",
       "      <td>-1.2672</td>\n",
       "      <td>-1.2769</td>\n",
       "      <td>-0.1334</td>\n",
       "      <td>-0.2890</td>\n",
       "      <td>-0.9264</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.2651</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>-0.5733</td>\n",
       "      <td>1.1497</td>\n",
       "      <td>-4.4118</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>1.9573</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>-0.2688</td>\n",
       "      <td>3.2499</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.2563</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>-1.5636</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>-0.2906</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>5.7554</td>\n",
       "      <td>1.0159</td>\n",
       "      <td>-0.7475</td>\n",
       "      <td>-0.5328</td>\n",
       "      <td>-0.8963</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>-0.2354</td>\n",
       "      <td>1.2082</td>\n",
       "      <td>-0.3503</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.6262</td>\n",
       "      <td>-0.1066</td>\n",
       "      <td>-0.9417</td>\n",
       "      <td>1.4577</td>\n",
       "      <td>-0.1797</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>-0.6604</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.2055</td>\n",
       "      <td>3.1045</td>\n",
       "      <td>-0.6147</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.4137</td>\n",
       "      <td>-0.8897</td>\n",
       "      <td>1.1796</td>\n",
       "      <td>0.6965</td>\n",
       "      <td>-2.1509</td>\n",
       "      <td>-0.4696</td>\n",
       "      <td>1.3644</td>\n",
       "      <td>1.8372</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-1.0257</td>\n",
       "      <td>1.4045</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>-0.0821</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TEXT:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos(2) healthy(94) and(9) active(102) ,(10) feisty(2058) kitten(84) found(71) in(20) neighbours(1026) '(232) garden(786) .(8) xxmaj(4) not(46) sure(301) of(19) sex(1939)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) hi(319) xxmaj(4) pet(149) xxmaj(4) lovers(572) !(43) xxmaj(4) this(49) is(14) my(32) first(231) posting(1619) and(9) i(16) need(123) help(161) !(43) 3(86) months(109) ago(261) we(36) befriended(5968) a(12) mother(142) stray(187) cat(50) with(23) 3(86) kittens(101) in(20) our(134) area(182) and(9) we(36) '(232) adopted(141) '(232) them(37) but(52) they(39) come(248) and(9) go(171) as(45) they(39) please(31) .(8) 2(62) months(109) ago(261) the(13) mother(142) gave(287) birth(244) to(11) a(12) litter(160) of(19) 7(286) cute(115) kittens(101) and(9) we(36) have(33) been(88) taking(367) care(79) of(19) the(13) mother(142) and(9) the(13) kittens(101) ever(625) since(195) in(20) our(134) home(25) ...(117) in(20) the(13) ground(2353) floor(1299) bathroom(4507) !(43) xxmaj(4) some(172) of(19) the(13) kittens(101) look(252) half(860) -(29) persian(804) with(23) long(157) hair(332) ,(10) xxunk(0) also(145) from(66) the(13) earlier(1557) stray(187) batch(2823) we(36) adopted(141) .(8) xxmaj(4) looking(82) for(15) good(60) caring(435) homes(378)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) to(11) be(27) spayed(200) on(53) /(56) 12(690) adorable(152) &amp;(67)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) birth(244) xxmaj(4) date(1109) :(41) xxmaj(4) oct(1752) 30th(5133) xxmaj(4) kitty(309) 1(108) ,(10) xxmaj(4) xxunk(0) -female(6549) xxmaj(4) sangat(672) comel(1017) and(9) xxunk(0) .(8) xxmaj(4) kitty(309) 2(62) ,(10) xxmaj(4) tootsie(7369) -(29) xxmaj(4) male(143) xxmaj(4) badan(3157) putih(1662) ,(10) tapi(1206) ekor(784) hitam(2135) and(9) telinga(4065) xxunk(0) -(29) xxunk(0) ,(10) macam(2396) kucing(300) 3(86) color(516) .(8) xxmaj(4) kitty(309) 3(86) ,(10) xxmaj(4) bo(6550) -male(4787) xxmaj(4) corak(5134) badan(3157) 3(86) tompok(3443) ,(10) he(21) is(14) one(89) of(19) the(13) most(276) quite(333) and(9) xxunk(0) kitty(309) of(19) the(13) litter(160) tapi(1206) jantan(1640) .(8) xxmaj(4) kitty(309) 4(107) ,(10) xxmaj(4) xxunk(0) -(29) xxmaj(4) female(127) xxmaj(4) macam(2396) abang(4509) xxunk(0) ,(10) she(17) is(14) very(26) xxunk(0) and(9) sweet(179) ,(10) have(33) 1(108) big(289) spot(928) behind(723) her(18) back(168) and(9) 1(108) super(365) cute(115) spot(928) belakang(2916) telinga(4065) .(8) xxmaj(4) kitty(309) 5(146) ,(10) xxmaj(4) missy(2621) -(29) xxmaj(4) female(127) .(8) xxmaj(4) she(17) is(14) very(26) curious(674) and(9) adventurous(1139) .(8) xxmaj(4) very(26) lovable(579) ,(10) with(23) sweet(179) voice(1433)</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) garfield(2060) is(14) a(12) very(26) large(956) cat(50) .(8) xxmaj(4) needs(189) daily(580) grooming(939) .(8) xxmaj(4) he(21) has(58) been(88) groomed(1850) short(341) and(9) deflea(1753) .(8) xxmaj(4) very(26) good(60) wt(2759) other(112) cats(98) .(8) xxmaj(4) calm(610) cat(50) .(8) xxmaj(4) he(21) is(14) neutered(259) ,(10) vaccinated(191) and(9) dewormed(207) .(8) xxmaj(4) must(114) be(27) kept(353) indoors(379) .(8) xxmaj(4) needs(189) grooming(939) and(9) brushing(4276) as(45) his(68) fur(239) is(14) very(26) soft(478) and(9) thick(1332) .(8) xxmaj(4) if(35) you(24) are(22) interested(77) to(11) adopt(64) xxmaj(4) garfield(2060) ,(10) please(31) give(69) me(34) a(12) call(74) or(51) sent(546) me(34) an(106) email(250) .(8) thank(173) you(24)</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabtext_db.show_batch(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 4., 4., 4., 4., 4., 4., 1., 2., 4., 2., 3., 1., 2., 4., 2., 1.,\n",
       "         1., 4., 4., 1., 4., 0., 2., 3., 3., 4., 3., 2., 1., 1., 2., 4., 3., 1.,\n",
       "         4., 3., 4., 4., 1., 4., 1., 2., 4., 1., 3., 4., 4., 1., 4., 3., 3., 4.,\n",
       "         2., 2., 3., 4., 4., 3., 4., 4., 4., 1.]), 64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = tabtext_db.one_batch(ds_type=DatasetType.Train)\n",
    "y1,len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 64, 64, torch.Size([64, 42]), torch.Size([64, 145]), torch.Size([64, 765]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x1),len(x1[0]),len(x1[1]),x1[0].shape,x1[1].shape,x1[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2, 127,   1,   2,   6,   7,   1,   2,   2,   1,   1,   1,   1,   3,\n",
       "         11,   2,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-3.8405e-01,  1.6226e+00, -4.5429e-01, -7.8942e-02, -3.6212e-01,\n",
       "        -2.9247e-01, -2.7784e-01, -6.6113e-01, -4.0079e-01, -3.3711e-01,\n",
       "        -3.9743e-01,  2.6175e-01, -1.0043e-01, -4.9071e-01, -1.4529e+00,\n",
       "         1.8988e-01, -3.5761e-01,  8.3176e-01,  8.3500e-01,  1.1769e-01,\n",
       "        -3.4273e-01, -3.4052e-01, -3.5297e-01, -3.0743e-01, -1.2816e-01,\n",
       "         1.3650e+00, -4.4736e-01, -1.3967e-01, -6.2311e-01, -3.6190e-01,\n",
       "        -3.4872e-01, -5.5261e-01, -6.3511e-01, -5.8299e-01,  1.1894e+00,\n",
       "         2.3167e-01,  5.2292e-01, -3.0570e-01,  7.9755e-01, -1.0707e+00,\n",
       "         1.4220e-02, -2.4799e-02, -2.1915e-01,  1.6981e-02, -3.5138e-01,\n",
       "        -9.9167e-01, -4.0593e-01, -4.3433e-01, -1.2103e+00,  4.8278e-01,\n",
       "         1.8928e-01,  2.1138e-01, -3.5382e-01, -1.0554e-01, -1.7935e-01,\n",
       "         8.3119e-01, -2.3576e-01, -4.6717e-01,  2.0447e+00,  8.2465e-01,\n",
       "        -8.5909e-01, -8.4936e-01,  8.7540e-01,  3.7276e-03,  3.2496e-02,\n",
       "         8.0510e-01,  2.3973e-02, -1.7139e-01, -5.9625e-01, -4.7785e-01,\n",
       "         9.5674e-02, -4.1886e-01, -2.4964e-01, -4.1713e-01, -1.5767e-01,\n",
       "        -2.2332e-01, -3.7101e-01, -2.5554e-01,  1.0250e+00, -2.1881e-02,\n",
       "        -4.5344e-01,  4.9453e-01, -3.5933e-01, -1.8016e-01,  2.3588e-02,\n",
       "        -5.2010e-01, -1.0907e+00,  1.9726e-03, -2.0530e-01,  8.0546e-01,\n",
       "        -3.8834e-01, -4.0034e-01, -1.3857e-01,  2.9612e-02, -3.5165e-01,\n",
       "         1.0352e-01, -3.6165e-01, -6.9214e-01, -1.5065e-01,  1.1286e+00,\n",
       "         4.1451e-01,  1.8111e-01, -1.7306e-01,  1.3053e+00,  7.6928e-01,\n",
       "         7.7184e-01, -1.1940e+00, -3.7124e-01,  1.0602e-01, -6.3509e-01,\n",
       "        -8.7040e-01,  7.7206e-02,  1.7431e-01,  5.1234e-01, -9.8306e-02,\n",
       "        -1.9211e-01, -4.5195e-01, -3.6565e-01,  1.4034e-01,  3.5408e-01,\n",
       "        -3.6176e-01, -3.5566e-01, -3.8977e-01,  1.3997e-01, -2.4459e-02,\n",
       "        -4.4168e-01, -2.4634e-01,  1.8121e-01, -3.4227e-01,  1.3407e-01,\n",
       "        -8.5293e-01,  8.0293e-01,  7.1145e-01, -1.4697e+00, -8.6814e-02,\n",
       "        -1.4814e-01, -2.5318e-01, -3.5367e-01, -4.7727e-01,  8.7204e-03,\n",
       "        -1.6491e-01,  2.2681e-01,  6.0800e-01,  8.1120e-02, -6.2505e-01])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0][0] #cats\n",
    "\n",
    "x1[1][0] #conts (normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step by step to create TabularText databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. create itemlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "itemlist: list of itembase, which contains independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'tmp_lm.pkl', bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TabularTextList.from_df(train, cat_names, cont_names, txt_cols, vocab=data_lm.vocab, procs=procs, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATS:\n",
      "['Type', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'State', 'm1_label_description', 'Is_Sterilized', 'Is_Free', 'IsMandarin', 'IsMalay', 'image_size_std_na', 'sentiment_document_score_na', 'metadata_metadata_color_pixelfrac_MEAN_na', 'sentiment_document_magnitude_na', 'image_size_sum_na', 'metadata_metadata_crop_conf_MEAN_na', 'metadata_metadata_crop_conf_SUM_na', 'metadata_metadata_color_pixelfrac_SUM_na', 'sentiment_magnitude_na', 'height_std_na', 'metadata_metadata_color_score_SUM_na', 'metadata_metadata_color_score_MEAN_na', 'metadata_metadata_crop_importance_MEAN_na', 'metadata_metadata_annots_score_MEAN_na', 'width_sum_na', 'image_size_mean_na', 'width_std_na', 'height_mean_na', 'metadata_metadata_crop_importance_SUM_na', 'width_mean_na', 'metadata_metadata_annots_score_SUM_na', 'sentiment_score_na', 'height_sum_na']\n",
      "TEXT COLS:\n",
      "['Description']\n",
      "PROCS:\n",
      "[<class 'fastai.tabular.transform.FillMissing'>, <class 'fastai.tabular.transform.Categorify'>, <class 'fastai.tabular.transform.Normalize'>]\n"
     ]
    }
   ],
   "source": [
    "print(f'CATS:\\n{il.cat_names}')\n",
    "# print(f'CONTS:\\n{il.cont_names}')\n",
    "print(f'TEXT COLS:\\n{il.text_cols}')\n",
    "print(f'PROCS:\\n{il.procs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                                                                         2\n",
       "Breed1                                                                                     299\n",
       "Breed2                                                                                       0\n",
       "Gender                                                                                       1\n",
       "Color1                                                                                       1\n",
       "Color2                                                                                       7\n",
       "Color3                                                                                       0\n",
       "MaturitySize                                                                                 1\n",
       "FurLength                                                                                    1\n",
       "Vaccinated                                                                                   2\n",
       "Dewormed                                                                                     2\n",
       "Sterilized                                                                                   2\n",
       "Health                                                                                       1\n",
       "State                                                                                    41326\n",
       "m1_label_description                                                                       cat\n",
       "Is_Sterilized                                                                            False\n",
       "Is_Free                                                                                  False\n",
       "IsMandarin                                                                               False\n",
       "IsMalay                                                                                  False\n",
       "image_size_std_na                                                                          NaN\n",
       "sentiment_document_score_na                                                                NaN\n",
       "metadata_metadata_color_pixelfrac_MEAN_na                                                  NaN\n",
       "sentiment_document_magnitude_na                                                            NaN\n",
       "image_size_sum_na                                                                          NaN\n",
       "metadata_metadata_crop_conf_MEAN_na                                                        NaN\n",
       "metadata_metadata_crop_conf_SUM_na                                                         NaN\n",
       "metadata_metadata_color_pixelfrac_SUM_na                                                   NaN\n",
       "sentiment_magnitude_na                                                                     NaN\n",
       "height_std_na                                                                              NaN\n",
       "metadata_metadata_color_score_SUM_na                                                       NaN\n",
       "                                                                   ...                        \n",
       "Quantity                                                                                     1\n",
       "m1_dominant_blue                                                                            21\n",
       "m1_bounding_confidence                                                                     0.8\n",
       "width_mean                                                                                 360\n",
       "TFIDF_sentiment_entities_13                                                         -0.0348534\n",
       "TFIDF_sentiment_entities_3                                                           0.0312046\n",
       "tolerates cold weather                                                                       0\n",
       "affectionate with family                                                                     0\n",
       "m1_bounding_importance                                                                       1\n",
       "exercise needs                                                                               0\n",
       "Breed1_m1_label_score_sum                                                              262.931\n",
       "State_Pop                                                                              541.132\n",
       "m1_vertex_y                                                                                479\n",
       "TFIDF_Description_9                                                                  0.0299928\n",
       "intelligence                                                                                 0\n",
       "m1_dominant_green                                                                           20\n",
       "metadata_metadata_annots_score_SUM                                                    0.973658\n",
       "State_m1_label_score_mean                                                             0.928307\n",
       "sentiment_score                                                                            0.3\n",
       "Fee                                                                                        100\n",
       "tendency to bark or howl                                                                     0\n",
       "TFIDF_Description_11                                                               -0.00296136\n",
       "VideoAmt                                                                                     0\n",
       "TFIDF_Description_2                                                                 -0.0278101\n",
       "TFIDF_sentiment_entities_7                                                           0.0265554\n",
       "TFIDF_Description_3                                                                -0.00606806\n",
       "potential for mouthiness                                                                     0\n",
       "height_sum                                                                                 480\n",
       "TFIDF_sentiment_entities_1                                                           0.0130525\n",
       "Description                                  Nibble is a 3+ month old ball of cuteness. He ...\n",
       "Name: 0, Length: 189, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "il.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ItemLists: train ItemList and validation itemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ils = il.split_by_idx(val_idxs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11939, 3051, PosixPath('/home/quantran/kwon/kaggle/new-comp/data'))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ils.train), len(ils.valid), ils.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     data_clas = (TextList.from_df(X_train,path, vocab=data_lm.vocab)\n",
    "#                  #grab all the text files in path\n",
    "#                  .split_by_idx(val_idxs[0])\n",
    "#                  #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "#                  .label_from_df(cols='AdoptionSpeed',label_cls=FloatList)\n",
    "#                  #label them all with their folders\n",
    "#                  .databunch(bs=32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: add y labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add y label -> becomes label lists\n",
    "\n",
    "this is essentially Pytorch Dataset, will be fed to model's forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = ils.label_from_df(cols='AdoptionSpeed',label_cls=FloatList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data_block.LabelList"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FloatItem 1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.valid.y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularText Type 2; Breed1 265; Breed2 0; Gender 2; Color1 6; Color2 0; Color3 0; MaturitySize 2; FurLength 2; Vaccinated 2; Dewormed 2; Sterilized 2; Health 1; State 41326; m1_label_description cat; Is_Sterilized False; Is_Free True; IsMandarin False; IsMalay False; image_size_std_na False; sentiment_document_score_na False; metadata_metadata_color_pixelfrac_MEAN_na False; sentiment_document_magnitude_na False; image_size_sum_na False; metadata_metadata_crop_conf_MEAN_na False; metadata_metadata_crop_conf_SUM_na False; metadata_metadata_color_pixelfrac_SUM_na False; sentiment_magnitude_na False; height_std_na False; metadata_metadata_color_score_SUM_na False; metadata_metadata_color_score_MEAN_na False; metadata_metadata_crop_importance_MEAN_na False; metadata_metadata_annots_score_MEAN_na False; width_sum_na False; image_size_mean_na False; width_std_na False; height_mean_na False; metadata_metadata_crop_importance_SUM_na False; width_mean_na False; metadata_metadata_annots_score_SUM_na False; sentiment_score_na False; height_sum_na False; m1_label_score 0.2183; RescuerID_IsFree_Mean 0.4945; State_m1_vertex_y_mean 0.4828; TFIDF_Description_1 0.1388; RescuerID_m1LabelScore_sum -0.3692; drooling potential -0.3074; TFIDF_sentiment_entities_11 0.3268; tolerates hot weather -0.3576; TFIDF_metadata_annots_top_desc_3 -0.1507; TFIDF_metadata_annots_top_desc_6 0.0299; TFIDF_sentiment_entities_14 -0.3161; image_size_std -0.3003; general health -0.4543; m1_dominant_pixel_frac 0.0303; RescuerID_Fee_Mean -0.3371; sentiment_document_score -0.7934; TFIDF_sentiment_entities_4 -0.3981; TFIDF_Description_4 -0.4783; TFIDF_metadata_annots_top_desc_14 2.7438; TFIDF_sentiment_entities_15 -0.0067; TFIDF_metadata_annots_top_desc_1 1.4372; Breed1_RescuerID_COUNT_sum -0.8704; State_RescuerID_COUNT_mean 0.7718; TFIDF_sentiment_entities_2 -0.2633; Age -0.4719; State_count 0.8051; metadata_metadata_color_pixelfrac_MEAN 0.3228; m1_dominant_red -1.0693; tolerates being alone -0.3423; PhotoAmt 0.6022; TFIDF_metadata_annots_top_desc_13 -0.5318; incredibly kid friendly dogs -0.3710; potential for weight gain -0.3616; TFIDF_Description_15 -0.4454; TFIDF_metadata_annots_top_desc_9 -2.4276; TFIDF_metadata_annots_top_desc_12 -0.0046; sensitivity level -0.3712; Breed1_RescuerID_COUNT_mean -0.8529; TFIDF_Description_14 0.4258; RescuerID_Fee_Sum -0.4003; TFIDF_sentiment_entities_5 -0.6531; TFIDF_metadata_annots_top_desc_15 0.3437; TFIDF_Description_8 1.3544; State_GDP -0.5201; sentiment_document_magnitude -0.8706; TFIDF_metadata_annots_top_desc_0 -0.7828; State_RescuerID_COUNT_sum 0.8350; image_size_sum 0.0306; dog friendly -0.3557; State_RescuerID_nunique 0.7693; TFIDF_metadata_annots_top_desc_7 -0.4922; RescuerID_age_mean 0.3578; metadata_metadata_crop_conf_MEAN -0.1055; TFIDF_Description_13 -0.9616; State_Area 0.2114; friendly toward strangers -0.4171; Breed1_m1_label_score_mean 0.6080; TFIDF_metadata_annots_top_desc_8 -1.1070; TFIDF_Description_7 0.1319; RescuerID_breed1_nunique -0.1105; metadata_metadata_crop_conf_SUM 0.5871; metadata_metadata_color_pixelfrac_SUM 0.7101; sentiment_magnitude -0.8303; height_std -0.6207; TFIDF_sentiment_entities_9 0.2029; TFIDF_metadata_annots_top_desc_5 0.0344; metadata_metadata_color_score_SUM 0.7023; easy to train -0.3619; TFIDF_Description_12 -0.6448; m1_dominant_score -0.3429; metadata_metadata_color_score_MEAN 0.5295; TFIDF_sentiment_entities_12 0.6081; Breed1_RescuerID_nunique -0.6231; State_Density -0.5830; TFIDF_sentiment_entities_10 -0.4767; TFIDF_Description_10 0.4375; TFIDF_metadata_annots_top_desc_2 -0.0607; energy level -0.3657; wanderlust potential -0.3514; name_length -1.1636; size -0.3427; TFIDF_Description_5 -0.2549; RescuerID_m1LabelScore_mean 0.2000; metadata_metadata_crop_importance_MEAN -0.0248; amount of shedding -0.4417; State_m1_label_score_sum 0.8055; good for novice owners -0.3530; metadata_metadata_annots_score_MEAN 0.1730; width_sum 0.5431; image_size_mean -0.6207; TFIDF_metadata_annots_top_desc_4 -0.1087; intensity -0.3593; TFIDF_Description_6 0.0041; RescuerID_COUNT -0.3704; Breed1_Age_mean -0.4907; TFIDF_sentiment_entities_8 1.0712; TFIDF_metadata_annots_top_desc_10 0.7317; Breed1_AdoptionSpeed_mean -0.6975; prey drive -0.3538; adapts well to apartment living -0.3405; Breed1_count -0.8591; easy to groom -0.4189; TFIDF_metadata_annots_top_desc_11 -0.1649; TFIDF_sentiment_entities_0 -0.1920; TFIDF_Description_0 -1.0823; pet friendly -0.2496; State_Age_mean -0.1802; kid friendly -0.2532; width_std -0.8797; RescuerID_m1VertexY_mean -0.6292; tendency to vocalize -0.2233; TFIDF_sentiment_entities_6 3.1878; height_mean -1.0736; potential for playfulness -0.4672; Breed1_m1_vertex_y_mean -0.6611; m1_vertex_x 0.1060; metadata_metadata_crop_importance_SUM 0.5891; Quantity -0.3974; m1_dominant_blue -0.7754; m1_bounding_confidence 0.1403; width_mean 0.0207; TFIDF_sentiment_entities_13 0.6769; TFIDF_sentiment_entities_3 0.6612; tolerates cold weather -0.3537; affectionate with family -0.4779; m1_bounding_importance 0.1400; exercise needs -0.3618; Breed1_m1_label_score_sum -0.8494; State_Pop 0.8318; m1_vertex_y -0.7940; TFIDF_Description_9 -0.4375; intelligence -0.4474; m1_dominant_green -0.8293; metadata_metadata_annots_score_SUM 0.6044; State_m1_label_score_mean 0.1177; sentiment_score -0.6921; Fee -0.2778; tendency to bark or howl -0.3487; TFIDF_Description_11 -0.6733; VideoAmt -0.1649; TFIDF_Description_2 -0.2367; TFIDF_sentiment_entities_7 -2.1347; TFIDF_Description_3 -0.2550; potential for mouthiness -0.3621; height_sum 0.1765; TFIDF_sentiment_entities_1 -0.3582; Text: xxbos healthy and active , feisty kitten found in neighbours ' garden . xxmaj not sure of sex ."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.valid.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  2, 127,   1,   2,   6,   1,   1,   2,   2,   2,   2,   2,   1,   3,\n",
       "          11,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]),\n",
       " tensor([ 0.2183,  0.4945,  0.4828,  0.1388, -0.3692, -0.3074,  0.3268, -0.3576,\n",
       "         -0.1507,  0.0299, -0.3161, -0.3003, -0.4543,  0.0303, -0.3371, -0.7934,\n",
       "         -0.3981, -0.4783,  2.7438, -0.0067,  1.4372, -0.8704,  0.7718, -0.2633,\n",
       "         -0.4719,  0.8051,  0.3228, -1.0693, -0.3423,  0.6022, -0.5318, -0.3710,\n",
       "         -0.3616, -0.4454, -2.4276, -0.0046, -0.3712, -0.8529,  0.4258, -0.4003,\n",
       "         -0.6531,  0.3437,  1.3544, -0.5201, -0.8706, -0.7828,  0.8350,  0.0306,\n",
       "         -0.3557,  0.7693, -0.4922,  0.3578, -0.1055, -0.9616,  0.2114, -0.4171,\n",
       "          0.6080, -1.1070,  0.1319, -0.1105,  0.5871,  0.7101, -0.8303, -0.6207,\n",
       "          0.2029,  0.0344,  0.7023, -0.3619, -0.6448, -0.3429,  0.5295,  0.6081,\n",
       "         -0.6231, -0.5830, -0.4767,  0.4375, -0.0607, -0.3657, -0.3514, -1.1636,\n",
       "         -0.3427, -0.2549,  0.2000, -0.0248, -0.4417,  0.8055, -0.3530,  0.1730,\n",
       "          0.5431, -0.6207, -0.1087, -0.3593,  0.0041, -0.3704, -0.4907,  1.0712,\n",
       "          0.7317, -0.6975, -0.3538, -0.3405, -0.8591, -0.4189, -0.1649, -0.1920,\n",
       "         -1.0823, -0.2496, -0.1802, -0.2532, -0.8797, -0.6292, -0.2233,  3.1878,\n",
       "         -1.0736, -0.4672, -0.6611,  0.1060,  0.5891, -0.3974, -0.7754,  0.1403,\n",
       "          0.0207,  0.6769,  0.6612, -0.3537, -0.4779,  0.1400, -0.3618, -0.8494,\n",
       "          0.8318, -0.7940, -0.4375, -0.4474, -0.8293,  0.6044,  0.1177, -0.6921,\n",
       "         -0.2778, -0.3487, -0.6733, -0.1649, -0.2367, -2.1347, -0.2550, -0.3621,\n",
       "          0.1765, -0.3582]),\n",
       " array([   2,   94,    9,  102,   10, 2058,   84,   71,   20, 1026,  232,  786,    8,    4,   46,  301,   19, 1939,\n",
       "           8])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.valid.x[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos healthy and active , feisty kitten found in neighbours ' garden . xxmaj not sure of sex .\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.valid.x[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   94,    9,  102,   10, 2058,   84,   71,   20, 1026,  232,  786,    8,    4,   46,  301,   19, 1939,\n",
       "          8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.valid.x.text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10131, 10131)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lls.train.x.vocab.itos), len(lls.valid.x.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = lls.add_test(TabularTextList.from_df(test, cat_names, cont_names, txt_cols,procs=procs, path=path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([  1, 164,   1,   1,   1,   1,   1,   2,   2,   2,   2,   2,   1,   3,\n",
       "          20,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "           1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1]),\n",
       " tensor([ 1.2947e-01, -1.5042e+00,  4.5436e-01,  2.7206e-01,  9.8155e-01,\n",
       "         -3.0743e-01, -2.2018e+00, -3.5761e-01, -1.5066e-01,  8.2639e-01,\n",
       "          7.5734e-01, -5.0052e-01, -4.5429e-01, -1.6898e-02,  1.2211e+00,\n",
       "          8.1120e-02,  2.1810e+00,  7.7454e-02,  3.0595e-01, -7.0714e-01,\n",
       "         -8.8790e-01,  9.0379e-01,  3.5707e-01,  9.7715e-01, -4.7187e-01,\n",
       "          6.5516e-01,  3.8508e-01, -1.4456e+00, -3.4227e-01, -2.6513e-01,\n",
       "          3.4878e-01, -3.7101e-01, -3.6165e-01,  1.9814e-02, -3.4950e-02,\n",
       "         -3.0038e+00, -3.7124e-01,  3.4839e-01, -3.9615e-01,  1.3585e+01,\n",
       "          3.0051e+00, -8.5804e-01,  1.3302e-01, -5.2010e-01, -3.0337e-01,\n",
       "          1.2564e+00,  4.7000e-01, -3.6701e-01, -3.5566e-01,  7.3454e-01,\n",
       "         -2.0052e-01,  6.0038e-01, -1.0554e-01, -1.7872e-01,  2.1138e-01,\n",
       "         -4.1713e-01, -5.0179e-02, -2.4897e-01,  4.7890e-01,  1.4651e+00,\n",
       "         -2.9058e-01, -1.8823e-01, -3.2160e-01,  1.0515e+00, -1.2768e+00,\n",
       "         -1.6294e+00, -2.4828e-01, -3.6190e-01, -1.1638e-01,  8.1047e-01,\n",
       "          3.3445e-01, -1.6341e-01,  8.9533e-01, -5.8299e-01, -1.7194e+00,\n",
       "         -5.8239e-01, -9.7646e-01, -3.6565e-01, -3.5138e-01, -5.0296e-01,\n",
       "         -3.4273e-01, -1.1290e+00,  1.0318e-01, -2.4799e-02, -4.4168e-01,\n",
       "          6.5533e-01, -3.5297e-01,  5.2574e-01, -3.3459e-01, -3.7104e-01,\n",
       "         -3.0755e-01, -3.5933e-01, -1.9790e-01,  9.7402e-01, -3.3743e-01,\n",
       "         -1.5633e-01, -2.3323e-01,  8.5256e-01, -3.5382e-01, -3.4052e-01,\n",
       "          9.2580e-01, -4.1886e-01,  2.6371e-01, -1.9195e-01, -8.1160e-01,\n",
       "         -2.4964e-01, -3.3024e-01, -2.5318e-01,  8.5186e-02,  1.2972e-01,\n",
       "         -2.2332e-01,  7.1174e-01, -6.9430e-01, -4.6717e-01,  6.2337e-01,\n",
       "          1.0602e-01, -2.8896e-01, -3.9743e-01, -1.1423e+00,  1.4034e-01,\n",
       "         -2.9325e-01,  2.1902e+00, -1.4240e+00, -3.5367e-01, -4.7785e-01,\n",
       "          1.3997e-01, -3.6176e-01,  9.1782e-01,  8.3176e-01, -7.9395e-01,\n",
       "          1.1644e+00, -4.4736e-01, -1.2780e+00, -2.7531e-01,  1.0792e-01,\n",
       "          1.5084e+00,  1.7751e+00, -3.4872e-01, -6.9412e-02, -1.6491e-01,\n",
       "         -2.5315e-03, -1.6638e-01,  1.3962e+00, -3.6212e-01, -4.0051e-01,\n",
       "          2.6148e-01]),\n",
       " array([   2,    4,  100,   14,  610,   15,   12,  279,   48,   10,   52,   21, 1485,   26,  868,  861,  156,    8,\n",
       "           4,   21,  266,  169, 3208,    9, 1189,    8])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.test.x[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj puppy is calm for a young dog , but he becomes very cheerful among people . xxmaj he likes being hugged and carried .'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([   2,    4,  100,   14,  610,   15,   12,  279,   48,   10,   52,   21, 1485,   26,  868,  861,  156,    8,\n",
       "          4,   21,  266,  169, 3208,    9, 1189,    8])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lls.test.x[0].text\n",
    "lls.test.x.text_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10131"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lls.test.x.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: build databunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collection of pytorch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = partial(mixed_tabular_pad_collate, pad_idx=1, pad_first=True)\n",
    "tabtext_db = lls.databunch(bs=64,collate_fn=collate_fn, no_check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = tabtext_db.one_batch(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 3., 4., 2., 3., 4., 3., 4., 3., 3., 4., 4., 4., 2., 4., 3., 4.,\n",
       "         1., 3., 3., 4., 3., 1., 2., 3., 2., 3., 2., 4., 4., 1., 2., 4., 3., 3.,\n",
       "         2., 1., 4., 2., 2., 2., 4., 4., 4., 4., 1., 3., 2., 4., 1., 1., 2., 2.,\n",
       "         2., 2., 2., 1., 4., 1., 2., 2., 2., 2.]), 64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "y1,len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([461]), torch.Size([461]), torch.Size([461]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = tabtext_db.one_batch(ds_type=DatasetType.Train)\n",
    "# noted that each piece text (paragraph) has been patched to have equal size (= size of longest texts)\n",
    "# this size can be different for each batch\n",
    "x1[2][0].shape,x1[2][1].shape,x1[2][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([484]), torch.Size([484]), torch.Size([484]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = tabtext_db.one_batch(ds_type=DatasetType.Valid)\n",
    "\n",
    "x1[2][0].shape,x1[2][1].shape,x1[2][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([141]), torch.Size([141]), torch.Size([141]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = tabtext_db.one_batch(ds_type=DatasetType.Test)\n",
    "\n",
    "x1[2][0].shape,x1[2][1].shape,x1[2][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabtext_db = get_tabulartext_databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TABULAR:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>State</th>\n",
       "      <th>m1_label_description</th>\n",
       "      <th>Is_Sterilized</th>\n",
       "      <th>Is_Free</th>\n",
       "      <th>IsMandarin</th>\n",
       "      <th>IsMalay</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM_na</th>\n",
       "      <th>sentiment_document_magnitude_na</th>\n",
       "      <th>height_sum_na</th>\n",
       "      <th>width_mean_na</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN_na</th>\n",
       "      <th>image_size_std_na</th>\n",
       "      <th>height_std_na</th>\n",
       "      <th>image_size_sum_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM_na</th>\n",
       "      <th>width_std_na</th>\n",
       "      <th>sentiment_document_score_na</th>\n",
       "      <th>width_sum_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM_na</th>\n",
       "      <th>sentiment_score_na</th>\n",
       "      <th>metadata_metadata_annots_score_SUM_na</th>\n",
       "      <th>height_mean_na</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN_na</th>\n",
       "      <th>sentiment_magnitude_na</th>\n",
       "      <th>metadata_metadata_color_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN_na</th>\n",
       "      <th>metadata_metadata_color_score_SUM_na</th>\n",
       "      <th>image_size_mean_na</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_8</th>\n",
       "      <th>State_RescuerID_nunique</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM</th>\n",
       "      <th>State_RescuerID_COUNT_mean</th>\n",
       "      <th>potential for playfulness</th>\n",
       "      <th>Breed1_RescuerID_COUNT_mean</th>\n",
       "      <th>TFIDF_Description_4</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>height_sum</th>\n",
       "      <th>affectionate with family</th>\n",
       "      <th>sensitivity level</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_10</th>\n",
       "      <th>RescuerID_IsFree_Mean</th>\n",
       "      <th>potential for weight gain</th>\n",
       "      <th>kid friendly</th>\n",
       "      <th>State_m1_label_score_mean</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_7</th>\n",
       "      <th>easy to groom</th>\n",
       "      <th>Breed1_AdoptionSpeed_mean</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>Age</th>\n",
       "      <th>TFIDF_sentiment_entities_15</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN</th>\n",
       "      <th>TFIDF_Description_7</th>\n",
       "      <th>image_size_std</th>\n",
       "      <th>drooling potential</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_15</th>\n",
       "      <th>RescuerID_m1LabelScore_mean</th>\n",
       "      <th>RescuerID_COUNT</th>\n",
       "      <th>TFIDF_Description_11</th>\n",
       "      <th>TFIDF_Description_9</th>\n",
       "      <th>TFIDF_Description_2</th>\n",
       "      <th>TFIDF_sentiment_entities_1</th>\n",
       "      <th>State_GDP</th>\n",
       "      <th>height_std</th>\n",
       "      <th>tendency to bark or howl</th>\n",
       "      <th>m1_dominant_green</th>\n",
       "      <th>tolerates hot weather</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_13</th>\n",
       "      <th>State_m1_vertex_y_mean</th>\n",
       "      <th>State_m1_label_score_sum</th>\n",
       "      <th>image_size_sum</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_11</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_6</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_5</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>Breed1_RescuerID_nunique</th>\n",
       "      <th>TFIDF_Description_14</th>\n",
       "      <th>TFIDF_sentiment_entities_12</th>\n",
       "      <th>State_RescuerID_COUNT_sum</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>TFIDF_Description_6</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN</th>\n",
       "      <th>TFIDF_Description_15</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM</th>\n",
       "      <th>TFIDF_Description_10</th>\n",
       "      <th>TFIDF_Description_12</th>\n",
       "      <th>TFIDF_sentiment_entities_6</th>\n",
       "      <th>width_std</th>\n",
       "      <th>TFIDF_sentiment_entities_7</th>\n",
       "      <th>good for novice owners</th>\n",
       "      <th>State_Age_mean</th>\n",
       "      <th>Breed1_count</th>\n",
       "      <th>TFIDF_Description_3</th>\n",
       "      <th>TFIDF_sentiment_entities_5</th>\n",
       "      <th>TFIDF_sentiment_entities_8</th>\n",
       "      <th>potential for mouthiness</th>\n",
       "      <th>State_Density</th>\n",
       "      <th>m1_dominant_score</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_1</th>\n",
       "      <th>RescuerID_m1LabelScore_sum</th>\n",
       "      <th>Breed1_Age_mean</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>m1_label_score</th>\n",
       "      <th>width_sum</th>\n",
       "      <th>RescuerID_Fee_Sum</th>\n",
       "      <th>incredibly kid friendly dogs</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_2</th>\n",
       "      <th>TFIDF_sentiment_entities_9</th>\n",
       "      <th>TFIDF_Description_0</th>\n",
       "      <th>RescuerID_m1VertexY_mean</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_4</th>\n",
       "      <th>RescuerID_breed1_nunique</th>\n",
       "      <th>TFIDF_sentiment_entities_4</th>\n",
       "      <th>general health</th>\n",
       "      <th>RescuerID_Fee_Mean</th>\n",
       "      <th>State_count</th>\n",
       "      <th>m1_dominant_pixel_frac</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_9</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>TFIDF_Description_1</th>\n",
       "      <th>TFIDF_sentiment_entities_10</th>\n",
       "      <th>adapts well to apartment living</th>\n",
       "      <th>metadata_metadata_annots_score_SUM</th>\n",
       "      <th>wanderlust potential</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN</th>\n",
       "      <th>State_Area</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>m1_bounding_confidence</th>\n",
       "      <th>m1_bounding_importance</th>\n",
       "      <th>RescuerID_age_mean</th>\n",
       "      <th>metadata_metadata_color_score_MEAN</th>\n",
       "      <th>amount of shedding</th>\n",
       "      <th>TFIDF_sentiment_entities_0</th>\n",
       "      <th>State_Pop</th>\n",
       "      <th>Fee</th>\n",
       "      <th>energy level</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN</th>\n",
       "      <th>TFIDF_Description_8</th>\n",
       "      <th>size</th>\n",
       "      <th>name_length</th>\n",
       "      <th>Breed1_m1_vertex_y_mean</th>\n",
       "      <th>Breed1_RescuerID_COUNT_sum</th>\n",
       "      <th>intensity</th>\n",
       "      <th>TFIDF_sentiment_entities_13</th>\n",
       "      <th>easy to train</th>\n",
       "      <th>m1_vertex_y</th>\n",
       "      <th>TFIDF_sentiment_entities_3</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_0</th>\n",
       "      <th>Breed1_m1_label_score_mean</th>\n",
       "      <th>TFIDF_sentiment_entities_14</th>\n",
       "      <th>tolerates cold weather</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_3</th>\n",
       "      <th>friendly toward strangers</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_12</th>\n",
       "      <th>tolerates being alone</th>\n",
       "      <th>exercise needs</th>\n",
       "      <th>m1_dominant_red</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>dog friendly</th>\n",
       "      <th>m1_dominant_blue</th>\n",
       "      <th>TFIDF_Description_5</th>\n",
       "      <th>metadata_metadata_color_score_SUM</th>\n",
       "      <th>prey drive</th>\n",
       "      <th>Breed1_m1_label_score_sum</th>\n",
       "      <th>TFIDF_Description_13</th>\n",
       "      <th>TFIDF_sentiment_entities_2</th>\n",
       "      <th>tendency to vocalize</th>\n",
       "      <th>m1_vertex_x</th>\n",
       "      <th>TFIDF_sentiment_entities_11</th>\n",
       "      <th>image_size_mean</th>\n",
       "      <th>pet friendly</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.4697</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>-0.1481</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>-0.2555</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.8246</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>-0.2925</td>\n",
       "      <td>2.0447</td>\n",
       "      <td>-0.1794</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>-0.1004</td>\n",
       "      <td>-0.6351</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>0.2618</td>\n",
       "      <td>-0.3898</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-0.3057</td>\n",
       "      <td>-0.2191</td>\n",
       "      <td>-0.5526</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>1.3053</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.7115</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>1.1894</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.5962</td>\n",
       "      <td>-0.4008</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>-0.1386</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>-0.3516</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>1.3650</td>\n",
       "      <td>-0.4773</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>-0.4519</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>0.1341</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>-0.4343</td>\n",
       "      <td>-0.0219</td>\n",
       "      <td>-0.1714</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>0.3541</td>\n",
       "      <td>0.8029</td>\n",
       "      <td>-0.3883</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>-0.0868</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.1397</td>\n",
       "      <td>-0.1577</td>\n",
       "      <td>-0.2053</td>\n",
       "      <td>-0.4534</td>\n",
       "      <td>0.4145</td>\n",
       "      <td>-0.0245</td>\n",
       "      <td>-0.4059</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>1.0250</td>\n",
       "      <td>0.0957</td>\n",
       "      <td>-0.6921</td>\n",
       "      <td>-0.0983</td>\n",
       "      <td>0.1899</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-1.0707</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>1.6226</td>\n",
       "      <td>1.1286</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.1921</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-1.1940</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.6351</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1731</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-1.0907</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>-0.9917</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>-0.1282</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-1.4529</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.8754</td>\n",
       "      <td>-0.3840</td>\n",
       "      <td>0.1743</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.2358</td>\n",
       "      <td>-1.2103</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>-0.2906</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.5321</td>\n",
       "      <td>-0.4279</td>\n",
       "      <td>-0.3601</td>\n",
       "      <td>-0.4726</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.2854</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.8336</td>\n",
       "      <td>-1.6276</td>\n",
       "      <td>-0.4172</td>\n",
       "      <td>0.1484</td>\n",
       "      <td>0.3084</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>-0.4253</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>-0.4285</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>1.3271</td>\n",
       "      <td>-0.2350</td>\n",
       "      <td>-0.6327</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.3805</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-1.2120</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.5598</td>\n",
       "      <td>-0.1267</td>\n",
       "      <td>-0.4984</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.8983</td>\n",
       "      <td>0.2813</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>-0.2651</td>\n",
       "      <td>-0.1799</td>\n",
       "      <td>-0.5302</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>-0.2890</td>\n",
       "      <td>-0.4695</td>\n",
       "      <td>-0.0370</td>\n",
       "      <td>-0.0780</td>\n",
       "      <td>-0.8797</td>\n",
       "      <td>-0.0823</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>1.0779</td>\n",
       "      <td>-0.1025</td>\n",
       "      <td>-0.0756</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.0415</td>\n",
       "      <td>-0.9103</td>\n",
       "      <td>-0.4291</td>\n",
       "      <td>-0.3480</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.1509</td>\n",
       "      <td>-0.6215</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>2.8821</td>\n",
       "      <td>-0.1281</td>\n",
       "      <td>-0.8658</td>\n",
       "      <td>-0.9649</td>\n",
       "      <td>-0.3941</td>\n",
       "      <td>0.2491</td>\n",
       "      <td>-0.5044</td>\n",
       "      <td>-0.0363</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>1.5084</td>\n",
       "      <td>-0.0327</td>\n",
       "      <td>-0.0876</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.2789</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-1.0736</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3725</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.5076</td>\n",
       "      <td>-0.3212</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.1921</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.2387</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>1.2307</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.0137</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.7940</td>\n",
       "      <td>-0.0930</td>\n",
       "      <td>0.6951</td>\n",
       "      <td>-0.0506</td>\n",
       "      <td>-0.1748</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.6126</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>1.1076</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.2298</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.6226</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>-0.3229</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.0698</td>\n",
       "      <td>-0.3165</td>\n",
       "      <td>-0.7902</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-1.1754</td>\n",
       "      <td>-0.0638</td>\n",
       "      <td>-0.8787</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2918</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>-0.2906</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>-0.5870</td>\n",
       "      <td>-0.0832</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>-1.1896</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-0.7636</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>0.5231</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-1.4452</td>\n",
       "      <td>-1.9456</td>\n",
       "      <td>-1.2234</td>\n",
       "      <td>2.0246</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>0.1296</td>\n",
       "      <td>-0.4188</td>\n",
       "      <td>0.3069</td>\n",
       "      <td>-0.2851</td>\n",
       "      <td>-0.3208</td>\n",
       "      <td>-0.1021</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>1.1303</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.6653</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.1840</td>\n",
       "      <td>-0.9426</td>\n",
       "      <td>1.7727</td>\n",
       "      <td>0.7867</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>-0.5358</td>\n",
       "      <td>0.5036</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>-0.2651</td>\n",
       "      <td>-0.4629</td>\n",
       "      <td>0.3817</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>-0.2890</td>\n",
       "      <td>0.6903</td>\n",
       "      <td>-0.9221</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>1.8219</td>\n",
       "      <td>1.0502</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>-0.6292</td>\n",
       "      <td>-0.5942</td>\n",
       "      <td>-2.5858</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.2573</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>-0.4194</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>-1.3036</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>-0.1591</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.0438</td>\n",
       "      <td>2.0938</td>\n",
       "      <td>0.2646</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>-0.1890</td>\n",
       "      <td>-0.0809</td>\n",
       "      <td>-0.4059</td>\n",
       "      <td>-0.3374</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>-0.2168</td>\n",
       "      <td>-1.4257</td>\n",
       "      <td>-0.5312</td>\n",
       "      <td>-1.4128</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.3162</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.5251</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.4410</td>\n",
       "      <td>0.1259</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>0.4891</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-1.1636</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>0.5761</td>\n",
       "      <td>-1.2823</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>-1.6274</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.6048</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>1.3340</td>\n",
       "      <td>1.5709</td>\n",
       "      <td>-0.2720</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>-0.7398</td>\n",
       "      <td>0.6558</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>1.8634</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.9160</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>1.1722</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.5476</td>\n",
       "      <td>0.7030</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>2.0939</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>-1.2904</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.0747</td>\n",
       "      <td>-0.3101</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-2.2685</td>\n",
       "      <td>0.5171</td>\n",
       "      <td>0.2592</td>\n",
       "      <td>-0.5185</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>-0.3027</td>\n",
       "      <td>-0.4016</td>\n",
       "      <td>2.1344</td>\n",
       "      <td>-0.0555</td>\n",
       "      <td>-0.2365</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>1.3879</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.6367</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>0.9293</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.4224</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>2.1406</td>\n",
       "      <td>0.8306</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>-1.0021</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>1.1804</td>\n",
       "      <td>-0.1991</td>\n",
       "      <td>-0.4045</td>\n",
       "      <td>-0.3825</td>\n",
       "      <td>1.1745</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>-0.6331</td>\n",
       "      <td>2.3631</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>-0.7575</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>-0.3794</td>\n",
       "      <td>-0.3384</td>\n",
       "      <td>-0.2296</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.5359</td>\n",
       "      <td>1.0718</td>\n",
       "      <td>-0.2984</td>\n",
       "      <td>-0.4237</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.8935</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.0260</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.3855</td>\n",
       "      <td>-0.3670</td>\n",
       "      <td>0.8819</td>\n",
       "      <td>-0.1576</td>\n",
       "      <td>-0.4059</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.1573</td>\n",
       "      <td>-0.9828</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.6212</td>\n",
       "      <td>0.2401</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>1.2100</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8887</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.5742</td>\n",
       "      <td>-0.6381</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.1918</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>0.5823</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>-0.2847</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-2.0471</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-1.0997</td>\n",
       "      <td>1.0864</td>\n",
       "      <td>-0.9472</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>1.2903</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.4185</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>0.6966</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.4474</td>\n",
       "      <td>-0.5345</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.1796</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.4287</td>\n",
       "      <td>-0.5579</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41401</td>\n",
       "      <td>dog</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.1619</td>\n",
       "      <td>-0.5691</td>\n",
       "      <td>1.4647</td>\n",
       "      <td>-0.7447</td>\n",
       "      <td>2.4369</td>\n",
       "      <td>1.0021</td>\n",
       "      <td>0.5909</td>\n",
       "      <td>3.2138</td>\n",
       "      <td>1.1869</td>\n",
       "      <td>2.2347</td>\n",
       "      <td>3.1264</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-0.0044</td>\n",
       "      <td>-1.9733</td>\n",
       "      <td>3.3099</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.0565</td>\n",
       "      <td>-0.2286</td>\n",
       "      <td>3.0073</td>\n",
       "      <td>-0.7230</td>\n",
       "      <td>-0.3969</td>\n",
       "      <td>2.0447</td>\n",
       "      <td>0.5270</td>\n",
       "      <td>0.7336</td>\n",
       "      <td>-0.0858</td>\n",
       "      <td>0.5223</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.3938</td>\n",
       "      <td>0.1355</td>\n",
       "      <td>-0.3317</td>\n",
       "      <td>-0.2050</td>\n",
       "      <td>-0.1971</td>\n",
       "      <td>-0.4391</td>\n",
       "      <td>0.7891</td>\n",
       "      <td>1.6741</td>\n",
       "      <td>1.3053</td>\n",
       "      <td>0.5750</td>\n",
       "      <td>-0.4703</td>\n",
       "      <td>2.5027</td>\n",
       "      <td>-0.3081</td>\n",
       "      <td>-0.7675</td>\n",
       "      <td>-0.7548</td>\n",
       "      <td>0.5065</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>-0.6482</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>-1.5099</td>\n",
       "      <td>0.3361</td>\n",
       "      <td>-0.6651</td>\n",
       "      <td>-0.9728</td>\n",
       "      <td>1.4695</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>-0.7559</td>\n",
       "      <td>-0.7703</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>1.0491</td>\n",
       "      <td>-1.0206</td>\n",
       "      <td>0.8606</td>\n",
       "      <td>0.2317</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>2.2585</td>\n",
       "      <td>0.8094</td>\n",
       "      <td>-1.3268</td>\n",
       "      <td>0.3676</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>1.1961</td>\n",
       "      <td>1.6893</td>\n",
       "      <td>0.4136</td>\n",
       "      <td>-0.7321</td>\n",
       "      <td>-0.3311</td>\n",
       "      <td>1.9911</td>\n",
       "      <td>-0.3562</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>1.0839</td>\n",
       "      <td>0.3541</td>\n",
       "      <td>1.6841</td>\n",
       "      <td>-0.7740</td>\n",
       "      <td>-0.5886</td>\n",
       "      <td>1.6328</td>\n",
       "      <td>0.8432</td>\n",
       "      <td>0.8943</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>-0.0120</td>\n",
       "      <td>1.0615</td>\n",
       "      <td>2.0402</td>\n",
       "      <td>0.7785</td>\n",
       "      <td>-0.7534</td>\n",
       "      <td>0.4209</td>\n",
       "      <td>-0.0512</td>\n",
       "      <td>-1.0589</td>\n",
       "      <td>-0.5513</td>\n",
       "      <td>1.0129</td>\n",
       "      <td>3.7023</td>\n",
       "      <td>1.5178</td>\n",
       "      <td>4.2284</td>\n",
       "      <td>-0.4402</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.9300</td>\n",
       "      <td>3.1376</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>-1.2829</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>-1.2222</td>\n",
       "      <td>2.1856</td>\n",
       "      <td>2.4835</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-1.9006</td>\n",
       "      <td>0.6543</td>\n",
       "      <td>-0.3708</td>\n",
       "      <td>-0.3942</td>\n",
       "      <td>-0.9220</td>\n",
       "      <td>2.0490</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>1.7956</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.5221</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.0979</td>\n",
       "      <td>0.5480</td>\n",
       "      <td>0.6377</td>\n",
       "      <td>-0.0664</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>1.6340</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>2.5849</td>\n",
       "      <td>2.5268</td>\n",
       "      <td>-0.5337</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>1.1839</td>\n",
       "      <td>-0.2710</td>\n",
       "      <td>-0.3920</td>\n",
       "      <td>1.0117</td>\n",
       "      <td>2.1532</td>\n",
       "      <td>-1.3362</td>\n",
       "      <td>0.2886</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.3022</td>\n",
       "      <td>-0.6351</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "TEXT:<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) maya(2040) is(14) the(13) queen(2592) in(20) the(13) house(96) .(8) xxmaj(4) she(17) 's(55) beautiful(222) ,(10) sweet(179) and(9) looks(421) blur(3844) at(30) times(456) .(8) xxmaj(4) when(78) you(24) hug(1466) her(18) ,(10) it(59) 's(55) like(95) hugging(5395) a(12) soft(478) feathered(9375) xxunk(0) .(8) xxmaj(4) always(263) ready(262) for(15) hugs(1683) and(9) kisses(1867) ,(10) this(49) lady(593) xxunk(0) in(20) tickling(9298) on(53) her(18) stomach(2471) and(9) neck(1385) .(8) xxmaj(4) she(17) will(38) lean(2917) over(324) to(11) ask(630) for(15) a(12) kiss(2635) on(53) her(18) nose(1081) and(9) seldom(1681) make(230) any(148) noise(1135) .(8) xxmaj(4) feeds(2046) only(93) on(53) dry(551) kibbles(401) .(8) xxmaj(4) easy(474) maintenance(2114) .(8) p(782) /(56) s(470) :(41) xxmaj(4) no(90) cage(322) confinement(2099) please(31) ,(10) except(654) in(20) incidences(4072) of(19) illness(1492) or(51) injuries(1510) .(8)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) he(21) was(40) nearly(1649) became(1740) a(12) road(346) kill(1941) before(197) me(34) and(9) my(32) wife(1272) and(9) also(145) some(172) xxmaj(4) good(60) xxmaj(4) samaritans(4307) help(161) to(11) rescue(357) him(42) .(8) xxmaj(4) very(26) affectionate(338) little(124) guy(846) !(43)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) these(140) kittens(101) were(151) born(264) at(30) my(32) grandmother(4719) 's(55) yard(1595) ,(10) last(358) week(325) i(16) found(71) the(13) mother(142) dead(1514) ,(10) maybe(955) hit(706) by(61) a(12) car(272) around(125) 5(146) minutes(2732) before(197) i(16) saw(596) her(18) .(8) xxmaj(4) the(13) kittens(101) are(22) almost(476) 2(62) months(109) old(65) .(8) xxmaj(4) they(39) know(219) how(273) to(11) drink(1094) milk(457) by(61) themselves(2035) .(8) xxmaj(4) just(126) need(123) to(11) clean(405) them(37) up(87) every(486) time(113) they(39) poop(1614) .(8) xxmaj(4) really(163) need(123) someone(165) to(11) adopt(64) as(45) i(16) already(159) have(33) 7(286) cats(98) at(30) home(25) and(9) the(13) milking(5380) mother(142) is(14) not(46) keen(1063) on(53) giving(400) them(37) milk(457) .(8)</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) xxmaj(4) found(71) him(42) a(12) drain(443) 2(62) weeks(180) ago(261) .(8) xxmaj(4) super(365) playful(76) and(9) gets(430) along(283) great(302) with(23) other(112) cats(98) .(8) xxmaj(4) very(26) smart(314) and(9) never(372) messes(4199) up(87) the(13) house(96) .(8) xxmaj(4) he(21) also(145) knows(431) how(273) to(11) use(464) the(13) litter(160) box(271) and(9) is(14) not(46) used(429) to(11) be(27) alone(411) ((44) especially(749) at(30) xxunk(0) u(209) adopt(64) him(42) and(9) find(135) out(155) that(57) he(21) is(14) not(46) really(163) the(13) kitten(84) u(209) want(196) ,(10) u(209) may(249) always(263) return(517) him(42) back(168) to(11) me(34) .(8) xxmaj(4) pls(158) email(250) me(34) if(35) interested(77) :)(183)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos(2) 20(812) xxmaj(4) april(791) update(480) :(41) minpin(6178) is(14) on(53) trial(3401) adoption(54) minpin(6178) was(40) found(71) on(53) a(12) busy(578) road(346) and(9) is(14) being(169) put(310) up(87) for(15) adoption(54) since(195) no(90) one(89) claimed(2650) her(18) .(8) xxmaj(4) she(17) 's(55) now(85) believed(1486) to(11) be(27) abandoned(208) due(217) to(11) her(18) cataract(4259) .(8) xxmaj(4) due(217) to(11) her(18) early(1410) cataract(4259) which(213) is(14) probably(692) xxunk(0) inherited(5286) ,(10) she(17) can(47) only(93) see(293) xxunk(0) from(66) her(18) right(330) eye(504) although(769) she(17) has(58) good(60) vision(7216) from(66) her(18) left(254) eye(504) .(8) xxmaj(4) this(49) would(116) caused(1805) her(18) to(11) panic(4944) and(9) start(945) protecting(5532) herself(824) by(61) xxunk(0) if(35) there(130) 's(55) any(148) sudden(4807) movement(3658) .(8) xxmaj(4) however(364) ,(10) when(78) she(17) knows(431) you(24) ,(10) get(150) used(429) to(11) your(111) voice(1433) ,(10) she(17) will(38) be(27) as(45) adorable(152) as(45) a(12) kitten(84) .(8) xxmaj(4) in(20) addition(1503) ,(10) due(217) to(11) her(18) cataract(4259) problem(498) and(9) her(18) not(46) being(169) well(128) socialized(3891) ,(10) she(17) will(38) start(945) barking(1489) whenever(952) there(130) 's(55) dogs(121) or(51) cats(98) near(256) her(18) so(63) she(17) may(249) not(46) be(27) suitable(395) to(11) multi(3954) -(29) pet(149) homes(378) or(51) has(58) to(11) be(27) monitored(4853) when(78) around(125) other(112) pets(356) .(8) xxmaj(4) as(45) she(17) was(40) likely(1515) abandoned(208) by(61) her(18) owner(92) and(9) probably(692) had(166) to(11) fight(1370) for(15) food(105) while(317) on(53) the(13) streets(449) ,(10) she(17) is(14) still(178) protective(1241) over(324) her(18) food(105) while(317) she(17) 's(55) eating(556) ,(10) so(63) it(59) 's(55) best(389) not(46) to(11) disturb(2506) her(18) when(78) there(130) 's(55) food(105) in(20) front(455) of(19) her(18) .(8) xxmaj(4) behavior(2074) :(41) -(29) xxmaj(4) she(17) 's(55) toilet(241) trained(137) ,(10) will(38) only(93) pee(640) and(9) poo(711) outside(336) the(13) house(96) .(8) -(29) xxmaj(4) she(17) can(47) walk(432) off(532) leash(718) ,(10) she(17) will(38) follow(407) the(13) xxunk(0) sound(1750) when(78) you(24) want(196) her(18) to(11) follow(407) .(8) -(29) healthy(94) and(9) fit(1260) ,(10) loves(119) to(11) be(27) with(23) humans(463) .(8) -(29) xxmaj(4) her(18) personality(764) and(9) attitude(2103) is(14) more(138) suitable(395) for(15) a(12) family(122) that(57) do(80) not(46) have(33) other(112) pets(356) but(52) the(13) adopting(328) family(122) has(58) to(11) be(27) patient(960) when(78) dealing(4880) with(23) minpin(6178) and(9) she(17) will(38) eventually(2282) listen(1812) and(9) be(27) an(106) extreme(4796) cute(115) girl(206) when(78) she(17) gets(430) to(11) know(219) you(24) better(376) .(8) xxmaj(4) the(13) adopter(110) should(342) be(27) active(102) on(53) xxmaj(4) facebook(1183) so(63) that(57) we(36) can(47) get(150) pictorial(6292) updates(886) on(53) the(13) dog(48) and(9) forgo(9587) the(13) need(123) for(15) physical(1978) visits(1239) to(11) check(433) on(53) her(18) .(8) xxmaj(4) by(61) the(13) way(462) ,(10) minpin(6178) was(40) xxunk(0) in(20) xxmaj(4) star(2737) newspaper(1720) on(53) 16(1642) xxmaj(4) february(1927) under(321) the(13) xxmaj(4) dog(48) xxmaj(4) talk(1462) xxunk(0) .(8) xxmaj(4) thank(173) you(24) ,(10) xxmaj(4) ellen(6117) xxmaj(4) xxunk(0) !(43) xxmaj(4) if(35) you(24) 're(277) interested(77) to(11) adopt(64) her(18) ,(10) please(31) sent(546) a(12) message(557) ((44) via(555) petfinder.my(3042) )(28) and(9) tell(836) me(34) about(133) yourself(563) especially(749) your(111) dog(48) ownership(2531) experience(439) and(9) do(80) expect(1675) an(106) interview(1282) .(8)</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabtext_db.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itemlist: list of itembase, which contains independent variables\n",
    "data = TabularList.from_df(train, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.tabular.data.TabularList"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_lists = data.split_by_idx(val_idxs[0]) # train ItemList and validation itemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.tabular.data.TabularList"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(item_lists.valid) # there is no label yet (e.g. item_lists.valid.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_lists = item_lists.label_from_df(cols=dep_var,label_cls = FloatList) # add y label -> label lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data_block.LabelList"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_lists.valid) # this is essentially Pytorch Dataset, will be fed to model's forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fastai.data_block.FloatList'>\n",
      "<class 'fastai.tabular.data.TabularList'>\n"
     ]
    }
   ],
   "source": [
    "print(type(label_lists.valid.y))\n",
    "print(type(label_lists.valid.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tab = get_databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>State</th>\n",
       "      <th>m1_label_description</th>\n",
       "      <th>Is_Sterilized</th>\n",
       "      <th>Is_Free</th>\n",
       "      <th>IsMandarin</th>\n",
       "      <th>IsMalay</th>\n",
       "      <th>image_size_mean_na</th>\n",
       "      <th>metadata_metadata_color_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM_na</th>\n",
       "      <th>image_size_std_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN_na</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN_na</th>\n",
       "      <th>image_size_sum_na</th>\n",
       "      <th>height_sum_na</th>\n",
       "      <th>sentiment_score_na</th>\n",
       "      <th>sentiment_document_score_na</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM_na</th>\n",
       "      <th>metadata_metadata_color_score_SUM_na</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM_na</th>\n",
       "      <th>height_std_na</th>\n",
       "      <th>width_sum_na</th>\n",
       "      <th>metadata_metadata_annots_score_SUM_na</th>\n",
       "      <th>width_mean_na</th>\n",
       "      <th>sentiment_document_magnitude_na</th>\n",
       "      <th>height_mean_na</th>\n",
       "      <th>sentiment_magnitude_na</th>\n",
       "      <th>width_std_na</th>\n",
       "      <th>image_size_mean</th>\n",
       "      <th>State_Pop</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_15</th>\n",
       "      <th>State_count</th>\n",
       "      <th>RescuerID_age_mean</th>\n",
       "      <th>m1_vertex_x</th>\n",
       "      <th>TFIDF_Description_3</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_11</th>\n",
       "      <th>RescuerID_m1LabelScore_sum</th>\n",
       "      <th>metadata_metadata_color_score_MEAN</th>\n",
       "      <th>State_m1_vertex_y_mean</th>\n",
       "      <th>Breed1_m1_vertex_y_mean</th>\n",
       "      <th>TFIDF_sentiment_entities_3</th>\n",
       "      <th>potential for weight gain</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM</th>\n",
       "      <th>friendly toward strangers</th>\n",
       "      <th>affectionate with family</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_1</th>\n",
       "      <th>TFIDF_sentiment_entities_1</th>\n",
       "      <th>TFIDF_Description_8</th>\n",
       "      <th>Breed1_AdoptionSpeed_mean</th>\n",
       "      <th>intensity</th>\n",
       "      <th>TFIDF_sentiment_entities_9</th>\n",
       "      <th>image_size_std</th>\n",
       "      <th>State_Density</th>\n",
       "      <th>TFIDF_Description_2</th>\n",
       "      <th>State_RescuerID_COUNT_sum</th>\n",
       "      <th>TFIDF_sentiment_entities_12</th>\n",
       "      <th>energy level</th>\n",
       "      <th>incredibly kid friendly dogs</th>\n",
       "      <th>easy to train</th>\n",
       "      <th>TFIDF_Description_0</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_12</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN</th>\n",
       "      <th>TFIDF_Description_7</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_5</th>\n",
       "      <th>Breed1_RescuerID_COUNT_mean</th>\n",
       "      <th>image_size_sum</th>\n",
       "      <th>TFIDF_Description_5</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_10</th>\n",
       "      <th>tolerates cold weather</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_3</th>\n",
       "      <th>Age</th>\n",
       "      <th>State_GDP</th>\n",
       "      <th>height_sum</th>\n",
       "      <th>State_Area</th>\n",
       "      <th>Breed1_Age_mean</th>\n",
       "      <th>Breed1_RescuerID_nunique</th>\n",
       "      <th>Breed1_count</th>\n",
       "      <th>TFIDF_sentiment_entities_13</th>\n",
       "      <th>m1_bounding_confidence</th>\n",
       "      <th>tolerates being alone</th>\n",
       "      <th>TFIDF_sentiment_entities_8</th>\n",
       "      <th>TFIDF_sentiment_entities_15</th>\n",
       "      <th>TFIDF_Description_1</th>\n",
       "      <th>good for novice owners</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>TFIDF_Description_4</th>\n",
       "      <th>TFIDF_Description_15</th>\n",
       "      <th>TFIDF_Description_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_13</th>\n",
       "      <th>adapts well to apartment living</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_8</th>\n",
       "      <th>exercise needs</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_7</th>\n",
       "      <th>tendency to vocalize</th>\n",
       "      <th>tendency to bark or howl</th>\n",
       "      <th>RescuerID_m1VertexY_mean</th>\n",
       "      <th>name_length</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_6</th>\n",
       "      <th>Breed1_RescuerID_COUNT_sum</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>TFIDF_Description_9</th>\n",
       "      <th>TFIDF_Description_12</th>\n",
       "      <th>TFIDF_sentiment_entities_2</th>\n",
       "      <th>TFIDF_sentiment_entities_7</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM</th>\n",
       "      <th>wanderlust potential</th>\n",
       "      <th>metadata_metadata_color_score_SUM</th>\n",
       "      <th>RescuerID_Fee_Mean</th>\n",
       "      <th>State_m1_label_score_mean</th>\n",
       "      <th>State_m1_label_score_sum</th>\n",
       "      <th>prey drive</th>\n",
       "      <th>RescuerID_breed1_nunique</th>\n",
       "      <th>TFIDF_sentiment_entities_11</th>\n",
       "      <th>tolerates hot weather</th>\n",
       "      <th>size</th>\n",
       "      <th>m1_dominant_blue</th>\n",
       "      <th>TFIDF_sentiment_entities_6</th>\n",
       "      <th>TFIDF_sentiment_entities_5</th>\n",
       "      <th>State_RescuerID_nunique</th>\n",
       "      <th>m1_dominant_red</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM</th>\n",
       "      <th>height_std</th>\n",
       "      <th>Fee</th>\n",
       "      <th>TFIDF_sentiment_entities_10</th>\n",
       "      <th>width_sum</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_2</th>\n",
       "      <th>State_Age_mean</th>\n",
       "      <th>TFIDF_Description_10</th>\n",
       "      <th>m1_dominant_green</th>\n",
       "      <th>potential for playfulness</th>\n",
       "      <th>RescuerID_m1LabelScore_mean</th>\n",
       "      <th>general health</th>\n",
       "      <th>sensitivity level</th>\n",
       "      <th>metadata_metadata_annots_score_SUM</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>Breed1_m1_label_score_mean</th>\n",
       "      <th>m1_vertex_y</th>\n",
       "      <th>m1_bounding_importance</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>State_RescuerID_COUNT_mean</th>\n",
       "      <th>TFIDF_Description_6</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_0</th>\n",
       "      <th>pet friendly</th>\n",
       "      <th>amount of shedding</th>\n",
       "      <th>potential for mouthiness</th>\n",
       "      <th>kid friendly</th>\n",
       "      <th>RescuerID_Fee_Sum</th>\n",
       "      <th>dog friendly</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>easy to groom</th>\n",
       "      <th>Breed1_m1_label_score_sum</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>TFIDF_sentiment_entities_0</th>\n",
       "      <th>RescuerID_COUNT</th>\n",
       "      <th>m1_dominant_score</th>\n",
       "      <th>TFIDF_sentiment_entities_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_9</th>\n",
       "      <th>width_std</th>\n",
       "      <th>TFIDF_Description_13</th>\n",
       "      <th>RescuerID_IsFree_Mean</th>\n",
       "      <th>m1_dominant_pixel_frac</th>\n",
       "      <th>m1_label_score</th>\n",
       "      <th>TFIDF_sentiment_entities_4</th>\n",
       "      <th>TFIDF_Description_11</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_4</th>\n",
       "      <th>drooling potential</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.3437</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.3578</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.2550</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.3692</td>\n",
       "      <td>0.5295</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>0.6612</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>1.4372</td>\n",
       "      <td>-0.3582</td>\n",
       "      <td>1.3544</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>0.2029</td>\n",
       "      <td>-0.3003</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.2367</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.6081</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-1.0823</td>\n",
       "      <td>0.3228</td>\n",
       "      <td>-0.0046</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>-0.2549</td>\n",
       "      <td>0.6022</td>\n",
       "      <td>0.7317</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>0.6769</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>1.0712</td>\n",
       "      <td>-0.0067</td>\n",
       "      <td>0.1388</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.6921</td>\n",
       "      <td>-0.7934</td>\n",
       "      <td>-0.4783</td>\n",
       "      <td>-0.4454</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.5318</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-1.1070</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.4922</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.6292</td>\n",
       "      <td>-1.1636</td>\n",
       "      <td>2.7438</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.4375</td>\n",
       "      <td>-0.6448</td>\n",
       "      <td>-0.2633</td>\n",
       "      <td>-2.1347</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>0.7023</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.3268</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.7754</td>\n",
       "      <td>3.1878</td>\n",
       "      <td>-0.6531</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>-1.0693</td>\n",
       "      <td>0.5891</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.5431</td>\n",
       "      <td>-0.0607</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>-0.8293</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>0.6044</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>-0.8706</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>-0.7940</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-1.0736</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>-0.7828</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.8303</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1920</td>\n",
       "      <td>-0.3704</td>\n",
       "      <td>-0.3429</td>\n",
       "      <td>-0.3161</td>\n",
       "      <td>-2.4276</td>\n",
       "      <td>-0.8797</td>\n",
       "      <td>-0.9616</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.2183</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>-0.6733</td>\n",
       "      <td>-0.1087</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4637</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>3.6141</td>\n",
       "      <td>-0.1767</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.5742</td>\n",
       "      <td>1.8634</td>\n",
       "      <td>-0.6810</td>\n",
       "      <td>0.8607</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-0.6611</td>\n",
       "      <td>1.0284</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>-0.0885</td>\n",
       "      <td>0.7019</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>2.1340</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.3692</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.7245</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>0.7125</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>2.4888</td>\n",
       "      <td>-1.0765</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.3374</td>\n",
       "      <td>-0.8529</td>\n",
       "      <td>-0.6528</td>\n",
       "      <td>1.3525</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-0.1980</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1506</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.7792</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4907</td>\n",
       "      <td>-0.6231</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>0.1229</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-2.0590</td>\n",
       "      <td>-0.6482</td>\n",
       "      <td>-0.4924</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>0.4081</td>\n",
       "      <td>0.8099</td>\n",
       "      <td>0.1877</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>-1.3740</td>\n",
       "      <td>0.4751</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>0.2930</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>1.8753</td>\n",
       "      <td>-1.5543</td>\n",
       "      <td>-0.6008</td>\n",
       "      <td>-0.8704</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.4062</td>\n",
       "      <td>-0.3706</td>\n",
       "      <td>0.5191</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>-0.7812</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8360</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.5044</td>\n",
       "      <td>0.4531</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.6990</td>\n",
       "      <td>-0.0304</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>-1.1127</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-1.4085</td>\n",
       "      <td>-0.6451</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.5516</td>\n",
       "      <td>-0.8891</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8588</td>\n",
       "      <td>2.2813</td>\n",
       "      <td>0.2639</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.9175</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>-0.4818</td>\n",
       "      <td>-1.3210</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.2380</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-0.8494</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>-0.4188</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>-1.1138</td>\n",
       "      <td>0.6537</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>-0.9848</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>-0.4286</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0028</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.3995</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.2022</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.5076</td>\n",
       "      <td>-0.6262</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>-0.1629</td>\n",
       "      <td>-0.3986</td>\n",
       "      <td>0.5448</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-0.2847</td>\n",
       "      <td>-0.1620</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>1.1361</td>\n",
       "      <td>-0.6953</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>-0.0747</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.0133</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.1304</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1268</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-1.3586</td>\n",
       "      <td>0.4910</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.4226</td>\n",
       "      <td>0.5189</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>-0.7621</td>\n",
       "      <td>-0.5670</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-0.0717</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.2530</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.8333</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4237</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>-0.0561</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>2.2419</td>\n",
       "      <td>-0.2833</td>\n",
       "      <td>-0.2455</td>\n",
       "      <td>-0.1314</td>\n",
       "      <td>-0.6529</td>\n",
       "      <td>-1.0581</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-2.5701</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.1511</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.1453</td>\n",
       "      <td>-0.2387</td>\n",
       "      <td>1.8355</td>\n",
       "      <td>1.5110</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.4636</td>\n",
       "      <td>-0.3830</td>\n",
       "      <td>-0.9716</td>\n",
       "      <td>-0.2103</td>\n",
       "      <td>-0.7978</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8444</td>\n",
       "      <td>0.2856</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.5044</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>0.4168</td>\n",
       "      <td>-0.1872</td>\n",
       "      <td>-0.0948</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>-0.8747</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>0.5216</td>\n",
       "      <td>0.6666</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8748</td>\n",
       "      <td>-0.9212</td>\n",
       "      <td>-0.5870</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>-1.0142</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.6268</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1922</td>\n",
       "      <td>-0.3994</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>-0.0280</td>\n",
       "      <td>2.2284</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>0.7347</td>\n",
       "      <td>-2.0504</td>\n",
       "      <td>0.1629</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>-0.1168</td>\n",
       "      <td>1.8687</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.7777</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>2.2769</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>-0.4410</td>\n",
       "      <td>-1.3585</td>\n",
       "      <td>1.2022</td>\n",
       "      <td>-1.9524</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-1.0954</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-0.2847</td>\n",
       "      <td>-0.1650</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>-0.6999</td>\n",
       "      <td>0.5773</td>\n",
       "      <td>-0.0747</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.0131</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.3268</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>0.1284</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.2182</td>\n",
       "      <td>0.6969</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.3232</td>\n",
       "      <td>0.0870</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>1.1023</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>-0.8100</td>\n",
       "      <td>-0.6903</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-4.0559</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.5266</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.8333</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4237</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>-0.0561</td>\n",
       "      <td>-0.2912</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>-0.2104</td>\n",
       "      <td>-0.5393</td>\n",
       "      <td>0.8651</td>\n",
       "      <td>0.1334</td>\n",
       "      <td>-0.0049</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>3.5829</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-1.6291</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>-0.4185</td>\n",
       "      <td>-1.1636</td>\n",
       "      <td>-1.0196</td>\n",
       "      <td>2.6744</td>\n",
       "      <td>-0.6373</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.7988</td>\n",
       "      <td>-0.8588</td>\n",
       "      <td>-0.9773</td>\n",
       "      <td>-0.2123</td>\n",
       "      <td>-0.7824</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.9066</td>\n",
       "      <td>-0.3371</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>-0.4059</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-1.2798</td>\n",
       "      <td>-0.1880</td>\n",
       "      <td>-0.0952</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>-1.5035</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.1286</td>\n",
       "      <td>-0.9422</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>-1.3678</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.2372</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8707</td>\n",
       "      <td>-1.8631</td>\n",
       "      <td>-0.2466</td>\n",
       "      <td>0.3221</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>-0.9989</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>-0.4003</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.2199</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>5.2294</td>\n",
       "      <td>-0.4188</td>\n",
       "      <td>-0.1210</td>\n",
       "      <td>-0.0274</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>0.7223</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>-0.1166</td>\n",
       "      <td>-0.9175</td>\n",
       "      <td>-0.1544</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>285</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.9143</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-0.2083</td>\n",
       "      <td>0.8051</td>\n",
       "      <td>0.6874</td>\n",
       "      <td>-0.6262</td>\n",
       "      <td>-0.4528</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.9369</td>\n",
       "      <td>0.4828</td>\n",
       "      <td>-2.1551</td>\n",
       "      <td>1.4577</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2906</td>\n",
       "      <td>0.9503</td>\n",
       "      <td>1.1497</td>\n",
       "      <td>1.1796</td>\n",
       "      <td>-0.2472</td>\n",
       "      <td>0.4262</td>\n",
       "      <td>-2.0008</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.2688</td>\n",
       "      <td>-0.3672</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>-0.5328</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>0.1754</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.3805</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>1.0159</td>\n",
       "      <td>-0.8897</td>\n",
       "      <td>-0.5733</td>\n",
       "      <td>-0.5306</td>\n",
       "      <td>-0.2651</td>\n",
       "      <td>1.1315</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>1.3882</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.2563</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>-1.3688</td>\n",
       "      <td>-1.2672</td>\n",
       "      <td>-0.3503</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>-0.2652</td>\n",
       "      <td>-0.6604</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>1.3644</td>\n",
       "      <td>-1.0257</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>1.1492</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-2.1509</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.8963</td>\n",
       "      <td>3.2499</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>-0.1066</td>\n",
       "      <td>-2.0212</td>\n",
       "      <td>1.4045</td>\n",
       "      <td>-0.9417</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>1.4700</td>\n",
       "      <td>0.6201</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>-0.4137</td>\n",
       "      <td>-0.2354</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.1797</td>\n",
       "      <td>1.3044</td>\n",
       "      <td>0.1177</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>0.8179</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.1334</td>\n",
       "      <td>-0.9264</td>\n",
       "      <td>0.9332</td>\n",
       "      <td>0.7693</td>\n",
       "      <td>0.7545</td>\n",
       "      <td>-0.2890</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>-0.1631</td>\n",
       "      <td>-0.4696</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>-0.1802</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>0.6965</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>0.1861</td>\n",
       "      <td>1.2087</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.2777</td>\n",
       "      <td>-0.9212</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>-0.0821</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0642</td>\n",
       "      <td>0.7718</td>\n",
       "      <td>0.2860</td>\n",
       "      <td>-1.0039</td>\n",
       "      <td>1.9573</td>\n",
       "      <td>3.1045</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>1.8372</td>\n",
       "      <td>5.7554</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.4923</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>-1.2769</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1917</td>\n",
       "      <td>0.1519</td>\n",
       "      <td>0.7169</td>\n",
       "      <td>0.3234</td>\n",
       "      <td>-4.4118</td>\n",
       "      <td>-0.8797</td>\n",
       "      <td>1.2082</td>\n",
       "      <td>-1.5636</td>\n",
       "      <td>0.2207</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>-0.7475</td>\n",
       "      <td>-0.6147</td>\n",
       "      <td>-0.2055</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tab.show_batch(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>State</th>\n",
       "      <th>m1_label_description</th>\n",
       "      <th>Is_Sterilized</th>\n",
       "      <th>Is_Free</th>\n",
       "      <th>IsMandarin</th>\n",
       "      <th>IsMalay</th>\n",
       "      <th>image_size_mean_na</th>\n",
       "      <th>metadata_metadata_color_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM_na</th>\n",
       "      <th>image_size_std_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN_na</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN_na</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN_na</th>\n",
       "      <th>image_size_sum_na</th>\n",
       "      <th>height_sum_na</th>\n",
       "      <th>sentiment_score_na</th>\n",
       "      <th>sentiment_document_score_na</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN_na</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM_na</th>\n",
       "      <th>metadata_metadata_color_score_SUM_na</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM_na</th>\n",
       "      <th>height_std_na</th>\n",
       "      <th>width_sum_na</th>\n",
       "      <th>metadata_metadata_annots_score_SUM_na</th>\n",
       "      <th>width_mean_na</th>\n",
       "      <th>sentiment_document_magnitude_na</th>\n",
       "      <th>height_mean_na</th>\n",
       "      <th>sentiment_magnitude_na</th>\n",
       "      <th>width_std_na</th>\n",
       "      <th>image_size_mean</th>\n",
       "      <th>State_Pop</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_15</th>\n",
       "      <th>State_count</th>\n",
       "      <th>RescuerID_age_mean</th>\n",
       "      <th>m1_vertex_x</th>\n",
       "      <th>TFIDF_Description_3</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_11</th>\n",
       "      <th>RescuerID_m1LabelScore_sum</th>\n",
       "      <th>metadata_metadata_color_score_MEAN</th>\n",
       "      <th>State_m1_vertex_y_mean</th>\n",
       "      <th>Breed1_m1_vertex_y_mean</th>\n",
       "      <th>TFIDF_sentiment_entities_3</th>\n",
       "      <th>potential for weight gain</th>\n",
       "      <th>metadata_metadata_crop_conf_SUM</th>\n",
       "      <th>friendly toward strangers</th>\n",
       "      <th>affectionate with family</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_1</th>\n",
       "      <th>TFIDF_sentiment_entities_1</th>\n",
       "      <th>TFIDF_Description_8</th>\n",
       "      <th>Breed1_AdoptionSpeed_mean</th>\n",
       "      <th>intensity</th>\n",
       "      <th>TFIDF_sentiment_entities_9</th>\n",
       "      <th>image_size_std</th>\n",
       "      <th>State_Density</th>\n",
       "      <th>TFIDF_Description_2</th>\n",
       "      <th>State_RescuerID_COUNT_sum</th>\n",
       "      <th>TFIDF_sentiment_entities_12</th>\n",
       "      <th>energy level</th>\n",
       "      <th>incredibly kid friendly dogs</th>\n",
       "      <th>easy to train</th>\n",
       "      <th>TFIDF_Description_0</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_12</th>\n",
       "      <th>metadata_metadata_annots_score_MEAN</th>\n",
       "      <th>TFIDF_Description_7</th>\n",
       "      <th>metadata_metadata_crop_importance_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_5</th>\n",
       "      <th>Breed1_RescuerID_COUNT_mean</th>\n",
       "      <th>image_size_sum</th>\n",
       "      <th>TFIDF_Description_5</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_10</th>\n",
       "      <th>tolerates cold weather</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_3</th>\n",
       "      <th>Age</th>\n",
       "      <th>State_GDP</th>\n",
       "      <th>height_sum</th>\n",
       "      <th>State_Area</th>\n",
       "      <th>Breed1_Age_mean</th>\n",
       "      <th>Breed1_RescuerID_nunique</th>\n",
       "      <th>Breed1_count</th>\n",
       "      <th>TFIDF_sentiment_entities_13</th>\n",
       "      <th>m1_bounding_confidence</th>\n",
       "      <th>tolerates being alone</th>\n",
       "      <th>TFIDF_sentiment_entities_8</th>\n",
       "      <th>TFIDF_sentiment_entities_15</th>\n",
       "      <th>TFIDF_Description_1</th>\n",
       "      <th>good for novice owners</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_document_score</th>\n",
       "      <th>TFIDF_Description_4</th>\n",
       "      <th>TFIDF_Description_15</th>\n",
       "      <th>TFIDF_Description_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_13</th>\n",
       "      <th>adapts well to apartment living</th>\n",
       "      <th>metadata_metadata_crop_conf_MEAN</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_8</th>\n",
       "      <th>exercise needs</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_7</th>\n",
       "      <th>tendency to vocalize</th>\n",
       "      <th>tendency to bark or howl</th>\n",
       "      <th>RescuerID_m1VertexY_mean</th>\n",
       "      <th>name_length</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_6</th>\n",
       "      <th>Breed1_RescuerID_COUNT_sum</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>TFIDF_Description_9</th>\n",
       "      <th>TFIDF_Description_12</th>\n",
       "      <th>TFIDF_sentiment_entities_2</th>\n",
       "      <th>TFIDF_sentiment_entities_7</th>\n",
       "      <th>metadata_metadata_color_pixelfrac_SUM</th>\n",
       "      <th>wanderlust potential</th>\n",
       "      <th>metadata_metadata_color_score_SUM</th>\n",
       "      <th>RescuerID_Fee_Mean</th>\n",
       "      <th>State_m1_label_score_mean</th>\n",
       "      <th>State_m1_label_score_sum</th>\n",
       "      <th>prey drive</th>\n",
       "      <th>RescuerID_breed1_nunique</th>\n",
       "      <th>TFIDF_sentiment_entities_11</th>\n",
       "      <th>tolerates hot weather</th>\n",
       "      <th>size</th>\n",
       "      <th>m1_dominant_blue</th>\n",
       "      <th>TFIDF_sentiment_entities_6</th>\n",
       "      <th>TFIDF_sentiment_entities_5</th>\n",
       "      <th>State_RescuerID_nunique</th>\n",
       "      <th>m1_dominant_red</th>\n",
       "      <th>metadata_metadata_crop_importance_SUM</th>\n",
       "      <th>height_std</th>\n",
       "      <th>Fee</th>\n",
       "      <th>TFIDF_sentiment_entities_10</th>\n",
       "      <th>width_sum</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_2</th>\n",
       "      <th>State_Age_mean</th>\n",
       "      <th>TFIDF_Description_10</th>\n",
       "      <th>m1_dominant_green</th>\n",
       "      <th>potential for playfulness</th>\n",
       "      <th>RescuerID_m1LabelScore_mean</th>\n",
       "      <th>general health</th>\n",
       "      <th>sensitivity level</th>\n",
       "      <th>metadata_metadata_annots_score_SUM</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>sentiment_document_magnitude</th>\n",
       "      <th>Breed1_m1_label_score_mean</th>\n",
       "      <th>m1_vertex_y</th>\n",
       "      <th>m1_bounding_importance</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>State_RescuerID_COUNT_mean</th>\n",
       "      <th>TFIDF_Description_6</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_0</th>\n",
       "      <th>pet friendly</th>\n",
       "      <th>amount of shedding</th>\n",
       "      <th>potential for mouthiness</th>\n",
       "      <th>kid friendly</th>\n",
       "      <th>RescuerID_Fee_Sum</th>\n",
       "      <th>dog friendly</th>\n",
       "      <th>sentiment_magnitude</th>\n",
       "      <th>easy to groom</th>\n",
       "      <th>Breed1_m1_label_score_sum</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>TFIDF_sentiment_entities_0</th>\n",
       "      <th>RescuerID_COUNT</th>\n",
       "      <th>m1_dominant_score</th>\n",
       "      <th>TFIDF_sentiment_entities_14</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_9</th>\n",
       "      <th>width_std</th>\n",
       "      <th>TFIDF_Description_13</th>\n",
       "      <th>RescuerID_IsFree_Mean</th>\n",
       "      <th>m1_dominant_pixel_frac</th>\n",
       "      <th>m1_label_score</th>\n",
       "      <th>TFIDF_sentiment_entities_4</th>\n",
       "      <th>TFIDF_Description_11</th>\n",
       "      <th>TFIDF_metadata_annots_top_desc_4</th>\n",
       "      <th>drooling potential</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>dog like mammal</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-0.8580</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>1.3962</td>\n",
       "      <td>0.2637</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.3344</td>\n",
       "      <td>0.4544</td>\n",
       "      <td>0.6234</td>\n",
       "      <td>-1.4240</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.2906</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.8879</td>\n",
       "      <td>0.2615</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.8526</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-1.2768</td>\n",
       "      <td>-0.5005</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>-0.1634</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.8116</td>\n",
       "      <td>0.3851</td>\n",
       "      <td>-3.0038</td>\n",
       "      <td>0.5257</td>\n",
       "      <td>0.4789</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-1.6294</td>\n",
       "      <td>0.3484</td>\n",
       "      <td>-0.3670</td>\n",
       "      <td>-1.1290</td>\n",
       "      <td>-0.2651</td>\n",
       "      <td>-0.2332</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.4719</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.4005</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3374</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>2.1902</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.1563</td>\n",
       "      <td>-0.7071</td>\n",
       "      <td>0.2721</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>1.5084</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>-0.3961</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.2490</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.2005</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>-0.5030</td>\n",
       "      <td>0.3059</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>0.9038</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>1.1644</td>\n",
       "      <td>-0.1164</td>\n",
       "      <td>0.9771</td>\n",
       "      <td>-0.1664</td>\n",
       "      <td>-0.1882</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.2483</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>-2.2018</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-1.1423</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>3.0051</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>-1.4456</td>\n",
       "      <td>-0.2890</td>\n",
       "      <td>1.0515</td>\n",
       "      <td>1.7751</td>\n",
       "      <td>-1.7194</td>\n",
       "      <td>-0.3346</td>\n",
       "      <td>-0.9765</td>\n",
       "      <td>-0.3302</td>\n",
       "      <td>-0.5824</td>\n",
       "      <td>-1.2780</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.2753</td>\n",
       "      <td>-0.2933</td>\n",
       "      <td>-0.3034</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>-0.7940</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-0.6943</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>-0.1979</td>\n",
       "      <td>1.2564</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>13.5854</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.3216</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.9178</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1920</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.8105</td>\n",
       "      <td>0.7573</td>\n",
       "      <td>-0.0349</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>-0.1787</td>\n",
       "      <td>-1.5042</td>\n",
       "      <td>-0.0169</td>\n",
       "      <td>0.1295</td>\n",
       "      <td>2.1810</td>\n",
       "      <td>-0.0694</td>\n",
       "      <td>-0.3075</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.6725</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>0.2022</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.5281</td>\n",
       "      <td>-0.1629</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>1.1741</td>\n",
       "      <td>0.4544</td>\n",
       "      <td>-0.2544</td>\n",
       "      <td>-0.4078</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>1.1361</td>\n",
       "      <td>3.2258</td>\n",
       "      <td>0.6227</td>\n",
       "      <td>-0.1303</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.3678</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>-0.1784</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.6646</td>\n",
       "      <td>1.2418</td>\n",
       "      <td>-0.0025</td>\n",
       "      <td>-0.1960</td>\n",
       "      <td>0.4113</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>1.0894</td>\n",
       "      <td>-0.7880</td>\n",
       "      <td>-0.7966</td>\n",
       "      <td>0.3811</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-0.0717</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>0.7317</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4328</td>\n",
       "      <td>0.4347</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.1009</td>\n",
       "      <td>0.3402</td>\n",
       "      <td>1.6720</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-1.0589</td>\n",
       "      <td>-0.9392</td>\n",
       "      <td>1.4378</td>\n",
       "      <td>-1.6418</td>\n",
       "      <td>-0.2801</td>\n",
       "      <td>-1.0581</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-2.5701</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.1511</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>-0.3708</td>\n",
       "      <td>1.8355</td>\n",
       "      <td>1.5110</td>\n",
       "      <td>-0.6687</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.7522</td>\n",
       "      <td>3.0860</td>\n",
       "      <td>-2.1455</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>-0.7415</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8205</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-1.1423</td>\n",
       "      <td>-0.2120</td>\n",
       "      <td>-0.3952</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>-1.3588</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.3877</td>\n",
       "      <td>-0.8072</td>\n",
       "      <td>0.1421</td>\n",
       "      <td>-0.3302</td>\n",
       "      <td>-2.8384</td>\n",
       "      <td>-1.2930</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8736</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>-1.0976</td>\n",
       "      <td>0.3092</td>\n",
       "      <td>-1.3245</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-1.7448</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>-0.1236</td>\n",
       "      <td>-1.0142</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>13.5854</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-1.0847</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1920</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>2.2284</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>2.7361</td>\n",
       "      <td>-1.5042</td>\n",
       "      <td>0.9165</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>-0.5776</td>\n",
       "      <td>-0.6904</td>\n",
       "      <td>-0.1430</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-0.0727</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>-0.4563</td>\n",
       "      <td>-0.3099</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.4544</td>\n",
       "      <td>-0.2544</td>\n",
       "      <td>1.1500</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>1.6160</td>\n",
       "      <td>1.9341</td>\n",
       "      <td>-1.3058</td>\n",
       "      <td>-0.1303</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.1416</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.3218</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>0.4304</td>\n",
       "      <td>-0.0893</td>\n",
       "      <td>-0.0607</td>\n",
       "      <td>-0.5348</td>\n",
       "      <td>0.2112</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-1.3249</td>\n",
       "      <td>-0.7880</td>\n",
       "      <td>-0.7103</td>\n",
       "      <td>-1.2680</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>0.2945</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.9054</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4328</td>\n",
       "      <td>0.4347</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.4747</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>0.6185</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.6921</td>\n",
       "      <td>-0.6477</td>\n",
       "      <td>-0.5700</td>\n",
       "      <td>-0.2450</td>\n",
       "      <td>0.3549</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>0.2593</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>-0.1066</td>\n",
       "      <td>-1.2202</td>\n",
       "      <td>-1.9742</td>\n",
       "      <td>-0.6687</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.7799</td>\n",
       "      <td>-0.4564</td>\n",
       "      <td>-1.0667</td>\n",
       "      <td>-0.7343</td>\n",
       "      <td>-0.8414</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8295</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>1.1505</td>\n",
       "      <td>-1.3543</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>1.0874</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>1.7751</td>\n",
       "      <td>-0.1785</td>\n",
       "      <td>-0.8072</td>\n",
       "      <td>-0.2598</td>\n",
       "      <td>-0.3302</td>\n",
       "      <td>1.3921</td>\n",
       "      <td>1.2051</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8754</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>-0.3034</td>\n",
       "      <td>0.3092</td>\n",
       "      <td>-0.7940</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>-1.0736</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>-0.6136</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>13.5854</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.3216</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1918</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>0.6868</td>\n",
       "      <td>-0.0665</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>-0.1873</td>\n",
       "      <td>-1.5042</td>\n",
       "      <td>-0.1472</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>-1.1432</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>266</td>\n",
       "      <td>252</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1.3099</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>1.8634</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>-1.9524</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>-0.9119</td>\n",
       "      <td>0.4544</td>\n",
       "      <td>-0.2544</td>\n",
       "      <td>-0.0465</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.2113</td>\n",
       "      <td>-0.1303</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.3014</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.2110</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>-0.6719</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3860</td>\n",
       "      <td>0.1361</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>1.1023</td>\n",
       "      <td>-0.7880</td>\n",
       "      <td>-0.5457</td>\n",
       "      <td>-0.3977</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>-4.0559</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.3077</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.8152</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.4328</td>\n",
       "      <td>0.4347</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>-0.1530</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.0635</td>\n",
       "      <td>0.1834</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.2269</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>-0.3136</td>\n",
       "      <td>-0.3758</td>\n",
       "      <td>-0.0049</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>3.5829</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-1.6291</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>-0.3708</td>\n",
       "      <td>-1.0196</td>\n",
       "      <td>2.6744</td>\n",
       "      <td>-0.6687</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>-0.0413</td>\n",
       "      <td>-0.7296</td>\n",
       "      <td>-0.8543</td>\n",
       "      <td>0.1569</td>\n",
       "      <td>-0.8245</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.8997</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>-0.5156</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>-0.0755</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>-0.5337</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>1.0908</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>-0.6451</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>-0.3302</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>-0.3356</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8723</td>\n",
       "      <td>2.2813</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.3092</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>-0.0600</td>\n",
       "      <td>-0.9989</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>13.5854</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1921</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>-0.2634</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>-1.5042</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>-0.0827</td>\n",
       "      <td>-0.2940</td>\n",
       "      <td>-0.1544</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>41326</td>\n",
       "      <td>dog breed</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.4882</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>-0.3974</td>\n",
       "      <td>-4.1919</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>-0.4798</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>-0.0452</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>-2.3545</td>\n",
       "      <td>0.4544</td>\n",
       "      <td>0.6234</td>\n",
       "      <td>-0.1984</td>\n",
       "      <td>-0.3616</td>\n",
       "      <td>-0.8757</td>\n",
       "      <td>-0.4171</td>\n",
       "      <td>-0.4779</td>\n",
       "      <td>-0.8894</td>\n",
       "      <td>-0.1251</td>\n",
       "      <td>-0.5454</td>\n",
       "      <td>0.8526</td>\n",
       "      <td>-0.3593</td>\n",
       "      <td>-0.9602</td>\n",
       "      <td>-0.2419</td>\n",
       "      <td>-0.5830</td>\n",
       "      <td>-0.2831</td>\n",
       "      <td>0.4700</td>\n",
       "      <td>1.8819</td>\n",
       "      <td>-0.3657</td>\n",
       "      <td>-0.3710</td>\n",
       "      <td>-0.3619</td>\n",
       "      <td>-0.3422</td>\n",
       "      <td>-2.1271</td>\n",
       "      <td>2.2889</td>\n",
       "      <td>-0.5256</td>\n",
       "      <td>-0.3505</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>-0.0520</td>\n",
       "      <td>0.3484</td>\n",
       "      <td>-0.7733</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>-0.8433</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>-0.3537</td>\n",
       "      <td>-0.1507</td>\n",
       "      <td>-0.2530</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>-0.7756</td>\n",
       "      <td>0.2114</td>\n",
       "      <td>-0.3374</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.9258</td>\n",
       "      <td>-0.0357</td>\n",
       "      <td>0.1403</td>\n",
       "      <td>-0.3423</td>\n",
       "      <td>-0.2526</td>\n",
       "      <td>1.5968</td>\n",
       "      <td>-0.2414</td>\n",
       "      <td>-0.3530</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.1375</td>\n",
       "      <td>-0.4440</td>\n",
       "      <td>0.7522</td>\n",
       "      <td>0.4512</td>\n",
       "      <td>-1.7660</td>\n",
       "      <td>-0.3405</td>\n",
       "      <td>-0.1055</td>\n",
       "      <td>-0.1738</td>\n",
       "      <td>-0.3618</td>\n",
       "      <td>-0.0700</td>\n",
       "      <td>-0.2233</td>\n",
       "      <td>-0.3487</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>-0.3715</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.9038</td>\n",
       "      <td>-0.4474</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>-0.1114</td>\n",
       "      <td>0.6047</td>\n",
       "      <td>-0.2552</td>\n",
       "      <td>-0.9942</td>\n",
       "      <td>-0.3514</td>\n",
       "      <td>-0.9544</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.6553</td>\n",
       "      <td>-0.3538</td>\n",
       "      <td>1.4651</td>\n",
       "      <td>1.7128</td>\n",
       "      <td>-0.3576</td>\n",
       "      <td>-0.3427</td>\n",
       "      <td>0.0806</td>\n",
       "      <td>-0.4978</td>\n",
       "      <td>-0.4795</td>\n",
       "      <td>0.7345</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>-0.8743</td>\n",
       "      <td>-0.6207</td>\n",
       "      <td>1.7751</td>\n",
       "      <td>1.2234</td>\n",
       "      <td>-0.8612</td>\n",
       "      <td>1.6379</td>\n",
       "      <td>-0.3302</td>\n",
       "      <td>0.0669</td>\n",
       "      <td>0.7713</td>\n",
       "      <td>-0.4672</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>-0.4543</td>\n",
       "      <td>-0.3712</td>\n",
       "      <td>-0.8754</td>\n",
       "      <td>-0.7328</td>\n",
       "      <td>-0.4736</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>0.8247</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>-0.3434</td>\n",
       "      <td>0.5720</td>\n",
       "      <td>-0.2496</td>\n",
       "      <td>-0.4417</td>\n",
       "      <td>-0.3621</td>\n",
       "      <td>-0.2532</td>\n",
       "      <td>13.5854</td>\n",
       "      <td>-0.3557</td>\n",
       "      <td>-0.5251</td>\n",
       "      <td>-0.4189</td>\n",
       "      <td>0.9178</td>\n",
       "      <td>-0.1649</td>\n",
       "      <td>-0.1920</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>-0.6760</td>\n",
       "      <td>0.4775</td>\n",
       "      <td>-0.0693</td>\n",
       "      <td>-0.1974</td>\n",
       "      <td>-0.4330</td>\n",
       "      <td>-1.5042</td>\n",
       "      <td>-0.4211</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>1.1720</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>-1.7202</td>\n",
       "      <td>-0.3074</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_tab.show_batch(ds_type=DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = data_tab.one_batch(ds_type=DatasetType.Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 2., 2., 3., 3., 1., 2., 3., 1., 2., 2., 4., 4., 4., 4., 0., 4., 1.,\n",
       "         2., 4., 4., 1., 2., 2., 2., 3., 2., 4., 2., 4., 4., 1., 2., 2., 2., 2.,\n",
       "         2., 4., 4., 4., 1., 0., 4., 1., 4., 4., 3., 1., 4., 3., 4., 3., 2., 2.,\n",
       "         1., 2., 2., 4., 2., 3., 2., 4., 4., 0.]), 64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 64, 64, torch.Size([64, 42]), torch.Size([64, 145]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([  2, 156,   1,   2,   2,   7,   1,   2,   1,   2,   1,   2,   1,   5,\n",
       "         11,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.8772,  0.3806, -0.4752, -0.2254, -0.4003, -0.6432, -0.1776, -1.1442,\n",
       "        -0.2383, -0.2890, -0.1921,  0.0399, -0.1515, -0.1222, -1.7294, -0.6351,\n",
       "         0.0047, -0.3074, -0.3371, -0.1649, -0.0869, -0.3537,  1.7047, -0.3509,\n",
       "        -0.3481,  0.0039, -0.2233, -0.2274, -1.2558,  0.0348, -0.2906, -0.2756,\n",
       "        -0.0248,  2.0888, -0.4779,  0.2517, -0.7411,  0.1933, -1.0589, -0.2842,\n",
       "         0.7111,  0.7377, -0.2677,  0.2045, -0.1867, -0.2487,  0.1403,  0.6815,\n",
       "        -0.0841, -0.8285, -0.4189,  0.2257, -0.0235,  0.5070, -0.3538,  0.2559,\n",
       "        -0.8663, -0.1055, -0.1869, -0.6207,  2.9088, -1.1089, -0.4543, -0.4417,\n",
       "        -0.1507, -0.6268, -0.4719, -0.0248, -0.3423, -0.9304, -0.4091, -1.5054,\n",
       "        -0.3657, -0.3514, -0.1035, -0.4544, -0.0917, -1.6352, -0.3712,  2.0422,\n",
       "         0.1400, -0.5742, -0.5056, -0.8795,  0.4945, -0.3621, -1.2210,  0.4495,\n",
       "         0.1493,  0.1636, -0.3557, -0.3619, -1.0004, -1.7584,  0.8247, -0.3427,\n",
       "        -0.3074,  1.5305, -0.4672, -0.8091,  0.9744, -0.2496, -0.0035, -1.2284,\n",
       "        -0.4171, -0.2778, -0.2854,  0.2206,  0.4350, -0.4474, -0.0440, -0.2140,\n",
       "        -0.8797,  0.4933, -1.7561,  0.7617,  1.0022, -0.4744, -0.2532, -0.7945,\n",
       "        -1.2833, -1.8753,  0.3577,  0.6658, -0.0141, -0.4087, -1.1747, -0.3405,\n",
       "         0.4147, -0.2651, -0.4966, -0.5870,  0.1009, -0.3560, -0.3710, -0.3487,\n",
       "        -0.3576, -0.3530, -0.3593, -1.2091, -1.9064, -0.3974, -0.3616, -0.0832,\n",
       "        -0.3618])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#label\n",
    "y1,len(y1)\n",
    "\n",
    "# each batch contains 2 columns: cats, conts\n",
    "len(x1),len(x1[0]),len(x1[1]),x1[0].shape,x1[1].shape\n",
    "\n",
    "x1[0][0] #cats\n",
    "\n",
    "x1[1][0] #conts (normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to text databunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_data = get_text_databunch(bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.TextClasDataBunch"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.TextList"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_data.valid_ds.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj for more pics go here : 9th xxmaj april edit : i have xxunk the adoption fee to xxup rm to include the fee i paid her breeder . xxmaj the reason i am now making reimbursement of the breeder 's fee compulsory is to help out xxmaj malaysian xxmaj dogs xxmaj deserve xxmaj better ( xxup mddb ) - they are currently in xxunk financial straits (</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj profile xxmaj xxunk : xxmaj name : xxmaj mama xxmaj kin xxmaj age : xxmaj adult cat . xxmaj probably 1 year and half . xxmaj condition : xxmaj healthy and xxmaj pregnant ( xxmaj probably will deliver in early xxmaj may ) . xxmaj believed that this is not the first time she is pregnant . xxmaj characteristics : xxmaj adorable . xxmaj soft . xxmaj very</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj my people kept many of us . xxmaj we were all semi long haired . i loved them because they were kind and fed us well . xxmaj the small ones would sometimes make us toys from rolled up news paper and bits of string . xxmaj they also let us out of the cage we lived in and i enjoyed running and chasing , climbing and rolling</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos [ xxmaj adopted 17 xxmaj november by xxmaj xxunk t ] \" i 'm a cute young standard xxmaj english bull terrier with lots of love to give . i love snuggling up for cuddles and belly rubs . xxmaj canine buddies make me super happy , i really love to play ! xxmaj like most of my xxunk , i 'm a xxunk xxunk ready to xxunk you</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj please send an sms before calling if possible because i might not pick up your calls when i 'm busy and might not call back unknown missed calls . xxmaj or you can send e - mail . xxmaj these two kittens are xxmaj beel and xxunk . xxmaj they are kids of on of the stray cats we feed . xxmaj we picked them up from the</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_data.show_batch(ds_type = DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.text.data.Text"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_data.valid_ds.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text xxbos healthy and active , feisty kitten found in neighbours ' garden . xxmaj not sure of sex ."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data.valid_ds.x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   94,    9,  102,   10, 2058,   84,   71,   20, 1026,  232,  786,    8,    4,   46,  301,   19, 1939,\n",
       "          8])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_data.valid_ds.x[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4., 3., 2., 1., 2., 3., 3., 4., 4., 2., 4., 3., 4., 2., 1., 1., 4., 4.,\n",
       "         4., 3., 2., 2., 4., 1., 3., 1., 1., 2., 3., 1., 1., 3., 3., 2., 4., 2.,\n",
       "         4., 2., 1., 2., 4., 4., 4., 2., 4., 4., 0., 2., 1., 4., 4., 4., 4., 0.,\n",
       "         4., 4., 4., 0., 2., 3., 1., 2., 2., 4.]), 64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1560])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = txt_data.one_batch(ds_type=DatasetType.Train)\n",
    "#label\n",
    "y1,len(y1)\n",
    "# each batch contains 1 column: text_ids\n",
    "\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2693,   13,  149,   44,    9,  132,   28,   19,  148, 1888, 1923,    8,\n",
       "           4,   36,   75,   72,   19,  134, 1192,    9,  616,   37,   95,  134,\n",
       "         290,  465,   10,   52,   36,  145, 3286,   37,  382, 7640,    8,    4,\n",
       "          36,  815,   95,   24,   11,  352,   19,   37,   45,  111,  290,  465,\n",
       "         139,    8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[0][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3., 2., 4., 2., 2., 4., 2., 3., 4., 3., 1., 4., 3., 1., 2., 2., 3., 1.,\n",
       "         3., 3., 4., 4., 2., 1., 4., 1., 4., 4., 3., 3., 1., 4.]), 32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1347])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = txt_data.one_batch(ds_type=DatasetType.Valid)\n",
    "#label\n",
    "y1,len(y1)\n",
    "# each batch contains 1 column: text_ids\n",
    "\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]), 32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1,y1 = txt_data.one_batch(ds_type=DatasetType.Test)\n",
    "#label\n",
    "y1,len(y1)\n",
    "# each batch contains 1 column: text_ids\n",
    "\n",
    "x1.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
